{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiDrelewicz/Aprendizaje-Automatico/blob/main/AA-II/notebooks/notebook_13_intro_a_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a Modelos de Lenguajes\n"
      ],
      "metadata": {
        "id": "CrE16YGZiqvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. ELMO [opcional]\n",
        "\n",
        "En el procesamiento de lenguaje natural (NLP), los modelos de lenguaje tradicionales como Word2Vec o GloVe representan cada palabra con un vector fijo, independientemente del contexto en el que aparece. Por ejemplo, la palabra \"banco\" tendrá el mismo embedding ya sea en \"banco suizo” o \"banco de la plaza\", lo cual limita la capacidad del modelo para capturar ambigüedades léxicas.\n",
        "\n",
        "ELMo (Embeddings from Language Models) propone una solución a esto utilizando un enfoque contextual basado en redes neuronales recurrentes bidireccionales (BiLSTM). En lugar de asignar un único vector por palabra, ELMo genera **representaciones dinámicas** que dependen del contexto completo de la oración. El input son embeddings estáticos y el output embeddings contextuales. El modelo está pre-entrenado pero puede ser fine-tuneado para tareas específicas.\n"
      ],
      "metadata": {
        "id": "9HBgZRobjSZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio: Explorando ELMo vs. GloVe\n",
        "\n",
        "En este ejercicio compararemos dos tipos de representaciones semánticas:\n",
        "-\tGloVe: embeddings estáticos (preentrenados, independientes del contexto).\n",
        "-\tELMo: embeddings contextuales (preentrenados, dependientes del contexto).\n",
        "\n",
        "\n",
        "1.\tDescargar  los vectores GloVe.\n",
        "2.\tUtilizar TensorFlow Hub para cargar el modelo preentrenado de ELMo.\n",
        "3.\tComparar cómo se representa una palabra ambigua (ej: bank) en distintos contextos.\n",
        "4.\tVisualizar los embeddings usando reducción de dimensionalidad (PCA o t-SNE)."
      ],
      "metadata": {
        "id": "HWzDkdQWndwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4toQ8OiawP6"
      },
      "outputs": [],
      "source": [
        "!pip install  -q tensorflow tensorflow_hub\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub # desgargamos elmo de tensorflow_hub\n",
        "import numpy as np\n",
        "import spacy # para procesar texto\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"ner\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture download_glove_vectors\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip --no-check-certificate\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "MiLO2e5kJww1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(download_glove_vectors)"
      ],
      "metadata": {
        "id": "g87GyXXfkzmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_vectors(filename):\n",
        "    embeddings = {}\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove_vectors('glove.6B.100d.txt')"
      ],
      "metadata": {
        "id": "No0MDHXCJyJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_glove_embedding(word):\n",
        "    return glove_embeddings.get(word, np.zeros((100,)))"
      ],
      "metadata": {
        "id": "ygsIbcN5NssK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo_layer = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", trainable=False, output_key='elmo', signature='tokens')"
      ],
      "metadata": {
        "id": "9_qRQ53TKsd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elmo_embedding(sentence, target_word=None):\n",
        "    doc = nlp(sentence)\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    if target_word is not None:\n",
        "      idx = tokens.index(target_word)\n",
        "\n",
        "    tokens_tensor = tf.convert_to_tensor([tokens])\n",
        "    sequence_len_tensor = tf.convert_to_tensor([len(tokens)])\n",
        "\n",
        "    embeddings = elmo_layer(inputs={'tokens': tokens_tensor, 'sequence_len': sequence_len_tensor})\n",
        "\n",
        "    if target_word is not None:\n",
        "      return embeddings.numpy()[0][idx]\n",
        "    else:\n",
        "      return embeddings.numpy()"
      ],
      "metadata": {
        "id": "139UGY09KyKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"apple\"\n",
        "print(\"GloVe embedding for 'apple':\", get_glove_embedding(word))\n",
        "print(\"ELMo embedding for 'apple':\", get_elmo_embedding(\"Apple is a fruit\")[0])"
      ],
      "metadata": {
        "id": "2yLl0y-RK4Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def visualize_contextual_embeddings(sentences, target_word):\n",
        "    glove_embedding = get_glove_embedding(target_word)\n",
        "    elmo_embeddings = [get_elmo_embedding(sentence, target_word) for sentence in sentences]\n",
        "\n",
        "    glove_embeddings = [glove_embedding for _ in sentences]\n",
        "\n",
        "    pca_glove = PCA(n_components=2)\n",
        "    glove_2d = pca_glove.fit_transform(glove_embeddings)\n",
        "\n",
        "    pca_elmo = PCA(n_components=2)\n",
        "    elmo_2d = pca_elmo.fit_transform(elmo_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        plt.scatter(glove_2d[i, 0], glove_2d[i, 1], color='blue', label='GloVe' if i == 0 else \"\")\n",
        "        plt.text(glove_2d[i, 0], glove_2d[i, 1], sentence, fontsize=9, color='blue')\n",
        "\n",
        "        plt.scatter(elmo_2d[i, 0], elmo_2d[i, 1], color='red', label='ELMo' if i == 0 else \"\")\n",
        "        plt.text(elmo_2d[i, 0], elmo_2d[i, 1], sentence, fontsize=9, color='red')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(f\"2D PCA of GloVe and ELMo embeddings for the word '{target_word}'\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bTG89cHJMZQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_sentences = [\n",
        "    \"I deposited money in the bank.\",\n",
        "    \"The boat drifted to the bank of the river.\",\n",
        "    \"We rested for a while on a wooden bank in the park.\",\n",
        "    \"Central bank printed money recklessly.\",\n",
        "    \"A bank in the pub.\",\n",
        "]\n",
        "\n",
        "visualize_contextual_embeddings(contextual_sentences, \"bank\")"
      ],
      "metadata": {
        "id": "umJORDlAMiiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Qué observa?\n",
        "\n",
        "> Su respuesta"
      ],
      "metadata": {
        "id": "fqzAj661ZXtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Self-Attention\n",
        "\n",
        "Adaptado de: https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF#scrollTo=oavQirdbhAK7\n",
        "\n",
        "En esta sección vamos a impelementar el mecanismo de self-attention visto en la clase teórica.\n",
        "\n",
        "¿Qué hace *self-attention*? El mecanismo de self-attention permite que cada vector de entrada en una secuencia “preste atención” a todos los demás (incluido a sí mismo) para construir una representación contextualizada.\n",
        "\n",
        "Dado un conjunto de vectores de entrada $\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$, el mecanismo produce vectores de salida $\\mathbf{z}_1, \\dots, \\mathbf{z}_n \\in \\mathbb{R}^{d’}$, donde cada salida $\\mathbf{z}_i$ es una combinación ponderada de todos los valores en la secuencia.\n",
        "\n",
        "Veamos cómo funciona paso a paso."
      ],
      "metadata": {
        "id": "bP71QRt_i0sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "TMA0dx9qw_Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, se proyectan las entradas a tres espacios distintos:\n",
        "- Queries: $\\mathbf{Q} = \\mathbf{XW}^Q$\n",
        "- Keys: $\\mathbf{K} = \\mathbf{XW}^K$\n",
        "- Values: $\\mathbf{V} = \\mathbf{XW}^V$\n",
        "\n",
        "donde $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d \\times d’}$ son matrices de pesos aprendibles.\n",
        "\n",
        "1. Consideremos un caso simple donde tenemos 3 vectores de $d=3$."
      ],
      "metadata": {
        "id": "axcWP2M0w_9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"700\">"
      ],
      "metadata": {
        "id": "or8uAjRkyXHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definir el input\n",
        "x = [\n",
        "  [1, 0, 1, 0], # Input 1\n",
        "  # Continuar ...\n",
        " ]\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "x"
      ],
      "metadata": {
        "id": "2pByU5cRyhjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cada vector tiene tres representaciones $k_i, q_i, v_i$ que se obtienen al multiplicar el vector de entrada por la matriz de peso correspondiente. En este caso usaremos una matriz de peso fija, pero en una red neuronal, estos se inicializan aleatoriamente (sampleando de distribución gaussiana, inicialización Xavier o Kaiming).\n",
        "\n",
        "Ejemplo: para obtener los `keys`, calculamos\n",
        "```\n",
        "               [0, 0, 1]\n",
        "[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\n",
        "[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\n",
        "[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]\n",
        "```\n",
        "\n",
        "\n",
        "![texto del enlace](https://miro.medium.com/max/1975/1*VPvXYMGjv0kRuoYqgFvCag.gif)"
      ],
      "metadata": {
        "id": "BI_5gIzFzLX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1. Definir los pesos, los consideraremos fijos\n",
        "w_key = [\n",
        "  [0, 0, 1],\n",
        "  [1, 1, 0],\n",
        "  [0, 1, 0],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_query = [\n",
        "  [1, 0, 1],\n",
        "  [1, 0, 0],\n",
        "  [0, 0, 1],\n",
        "  [0, 1, 1]\n",
        "]\n",
        "w_value = [\n",
        "  [0, 2, 0],\n",
        "  [0, 3, 0],\n",
        "  [1, 0, 3],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
        "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
        "w_value = torch.tensor(w_value, dtype=torch.float32)\n",
        "\n",
        "print(\"Weights for key: \\n\", w_key)\n",
        "print(\"Weights for query: \\n\", w_query)\n",
        "print(\"Weights for value: \\n\", w_value)"
      ],
      "metadata": {
        "id": "ZUhxG1sdyrsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Calculamos k_i, v_i, q_i\n",
        "\n",
        "keys = # TODO\n",
        "querys = #TODO\n",
        "values = #TODO\n",
        "\n",
        "print(\"Keys: \\n\", keys)\n",
        "# RTA:\n",
        "# tensor([[0., 1., 1.],\n",
        "#         [4., 4., 0.],\n",
        "#         [2., 3., 1.]])\n",
        "\n",
        "print(\"Querys: \\n\", querys)\n",
        "# RTA:\n",
        "# tensor([[1., 0., 2.],\n",
        "#         [2., 2., 2.],\n",
        "#         [2., 1., 3.]])\n",
        "print(\"Values: \\n\", values)\n",
        "# RTA:\n",
        "# tensor([[1., 2., 3.],\n",
        "#         [2., 8., 0.],\n",
        "#         [2., 6., 3.]])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QuN05Zu30s9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez que proyectamos $\\mathbf{X}$ en los espacios de $\\mathbf{K, Q, V}$, podemos calcular el attention score, dado por el producto punto entre cada query y todas las keys:\n",
        "$$\n",
        "\\text{Scores} = \\mathbf{Q} \\mathbf{K}^\\top\n",
        "$$"
      ],
      "metadata": {
        "id": "5LxbHN2k1RGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Veamos cómo lo calculamos en nuestro ejemplo. Para obtener los *scores de attention*, tomamos el primer `query` (rojo) y todos los `keys` (naranja), incluído a sí mismo; y computamos el producto punto entre cada uno:\n",
        "```\n",
        "               [0, 4, 2]\n",
        "[1, 0, 2] x [1, 4, 3] = [2, 4, 4]\n",
        "               [1, 0, 1]\n",
        "```\n",
        "  ![texto alternativo](https://miro.medium.com/max/1973/1*u27nhUppoWYIGkRDmYFN2A.gif)\n",
        "\n",
        "Este procedimiento se repite para todos los queries.\n",
        "\n",
        "Nota: *La operación anterior se conoce como dot-product attention, una de varias funciones de attention. Otras funciones de attention incluyen el scaled-dot-product y la función aditiva (o de concatenación).*            "
      ],
      "metadata": {
        "id": "GbA-_HMm1wSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Calculamos attention score\n",
        "\n",
        "attn_scores = #TODO\n",
        "print(attn_scores)\n",
        "\n",
        "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
        "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
        "#         [ 4., 12., 10.]]) # attention scores from Query 3"
      ],
      "metadata": {
        "id": "E9Swsydj3GQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los siguientes pasos son:\n",
        "- **Normalización**. Se aplica la función softmax para convertir los scores en probabilidades:\n",
        "$$\n",
        "\\mathbf{A} = \\text{softmax}(\\mathbf{QK}^\\top)\n",
        "$$\n",
        "- **Ponderación**. Se multiplica cada vector de atención por los values para obtener los valores ponderados:\n",
        "$$\n",
        "\\mathbf{Z} = \\mathbf{A} \\mathbf{V}\n",
        "$$\n",
        "donde $\\mathbf{A}$ es la matriz de atención, donde cada fila $\\mathbf{A}_i$ contiene los pesos softmax que indica cuánta atención debería prestar la entrada $i$ a todas las otras entradas $j$ de la fila."
      ],
      "metadata": {
        "id": "d2RbrRCC3TMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Cómputo de softmax. Si consideramos el primer vector $x_1$, el *softmax* para los attention score obtenidos se computa de la siguiente forma:\n",
        "```\n",
        "softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\n",
        "```\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*jf__2D8RNCzefwS0TP1Kyg.gif)"
      ],
      "metadata": {
        "id": "9D9TL0Uz3vGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Computar softmax para todos los ejemplos\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "attn_scores_softmax = # TODO\n",
        "print(attn_scores_softmax)\n",
        "\n",
        "# tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
        "#         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
        "#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
        "\n",
        "# Para poder leerlo más fácil, usemos estos valores.\n",
        "attn_scores_softmax = [\n",
        "  [0.0, 0.5, 0.5],\n",
        "  [0.0, 1.0, 0.0],\n",
        "  [0.0, 0.9, 0.1]\n",
        "]\n",
        "attn_scores_softmax = torch.tensor(attn_scores_softmax)\n",
        "print(attn_scores_softmax)\n",
        "print(attn_scores_softmax.shape)"
      ],
      "metadata": {
        "id": "QnHc9eGW3trt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Ponderamos los *values* con los score de attention normalizados para obtener values ponderados $z_i$.\n",
        "Veamos un ejemplo primero con $x_1$. ![texto alternativo](https://miro.medium.com/max/1973/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)\n",
        "Los attention scores luego del softmax para cada input (azul) son multiplicados con sus respectivos *values* (violeta). Esto resulta en tres vectores ponderados (amarillo):\n",
        "```\n",
        "1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\n",
        "2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\n",
        "3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\n",
        "```\n",
        "Luego, tomamos estos tres vectores ponderados y los sumamos:\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*1je5TwhVAwwnIeDFvww3ew.gif)\n",
        "\n",
        "  ```\n",
        "     [0.0, 0.0, 0.0]\n",
        "  + [1.0, 4.0, 0.0]\n",
        "  + [1.0, 3.0, 1.5]\n",
        "  -----------------\n",
        "  = [2.0, 7.0, 1.5]\n",
        "  ```\n",
        "\n",
        "  El vector resultante ```[2.0, 7.0, 1.5]``` (verde oscuro) es $z_1$, que está dada por la representación del query del input 1 y su interacción con todos los otros $keys$ incluído sí mismo.\n"
      ],
      "metadata": {
        "id": "8r_cv7aC4yUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. values ponderados\n",
        "\n",
        "weighted_values = #TODO\n",
        "print(weighted_values)\n",
        "\n",
        "outputs = #TODO\n",
        "print(outputs)\n",
        "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
        "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
        "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
      ],
      "metadata": {
        "id": "Xcr8D7vp6tBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estos últimos vectores $z_i$ son los embeddings contextuales."
      ],
      "metadata": {
        "id": "8cUBHBLpBycO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. BERT\n"
      ],
      "metadata": {
        "id": "D-UaOAlPjW-7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmtWNSpiwGv-"
      },
      "source": [
        "### 3.1 Tokenizador\n",
        "\n",
        "¿Cómo funciona el tokenizador de BERT?\n",
        "\n",
        "- **Enfoque de tokenización**: BERT utiliza un tokenizador llamado WordPiece. Esto significa que divide el texto en unidades más pequeñas llamadas subpalabras. Por ejemplo, una palabra poco frecuente puede dividirse en fragmentos comunes. Esto permite al modelo manejar palabras desconocidas y reducir el tamaño del vocabulario.\n",
        "\n",
        "- **Entrenamiento del vocabulario**:\n",
        "El tokenizador se entrena de manera no supervisada sobre un gran corpus de texto. Aprende un vocabulario de subpalabras (usualmente de unas 30.000) que representan bien el idioma.\n",
        "\t- Objetivo: El algoritmo de entrenamiento busca minimizar el número de tokens necesarios para representar el corpus, asegurando que las unidades resultantes capturen patrones lingüísticos significativos.\n",
        "\t- Ejemplo: Una palabra como unhappiness puede dividirse como [\"un\", \"##happiness\"] o incluso como [\"un\", \"##hap\", \"##piness\"], dependiendo del vocabulario aprendido.\n",
        "- **Rol en el preentrenamiento:**\n",
        "Durante el preentrenamiento, el tokenizador convierte texto crudo en identificadores de tokens que el modelo puede procesar. Esta conversión afecta directamente:\n",
        "\t- Masked Language Modeling (MLM): cómo se enmascaran las palabras.\n",
        "\t- Next Sentence Prediction (NSP): cómo se separan y combinan las oraciones.\n",
        "\n",
        "El diseño del tokenizador y su vocabulario impactan directamente en la capacidad de BERT para representar fenómenos lingüísticos complejos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOGNjsx0wGwF"
      },
      "outputs": [],
      "source": [
        "# 1. Cargar el tokenizador de BERT y tokenizar una oración, qué observa?\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "example_text = # COMPLETAR\n",
        "\n",
        "\n",
        "encoded = # COMPLETAR\n",
        "tokens = # COMPLETAR\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(example_text)\n",
        "print(\"\\nTokenized Output:\")\n",
        "print(tokens)\n",
        "print(\"\\nToken IDs:\")\n",
        "print(encoded['input_ids'].squeeze().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens especiales en BERT\n",
        "print(\"Special Tokens in the Tokenizer:\")\n",
        "print(\"CLS token:\", tokenizer.cls_token)\n",
        "print(\"SEP token:\", tokenizer.sep_token)\n",
        "print(\"PAD token:\", tokenizer.pad_token)\n",
        "print(\"MASK token:\", tokenizer.mask_token)"
      ],
      "metadata": {
        "id": "QnOWq1ShEYBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms40L5VJwGwG"
      },
      "source": [
        "¿Qué son los tokens especiales en BERT?\n",
        "\n",
        "- [CLS] (Token de Clasificación):\n",
        "  - Se coloca al inicio de cada secuencia.\n",
        "  -\tSe utiliza como representación de toda la entrada.\n",
        "  -\tSu estado oculto final se suele pasar a un clasificador para tareas como análisis de sentimiento.\n",
        "\n",
        "- [SEP] (Token Separador):\n",
        "\t- Se utiliza para separar diferentes segmentos dentro de la entrada.\n",
        "\t-\tEs esencial en tareas que involucran pares de oraciones (por ejemplo, preguntas y respuestas, predicción de la siguiente oración).\n",
        "\t-\tAyuda al modelo a entender dónde termina una oración y comienza otra.\n",
        "- [PAD] (Token de Relleno):\n",
        "\t- Se agrega a las secuencias para asegurar que todas tengan la misma longitud en un lote (batch).\n",
        "\t- Estos tokens se ignoran durante el procesamiento (usando una máscara de atención), ya que no representan contenido significativo.\n",
        "-\t[MASK] (Token de Enmascaramiento):\n",
        "\t-\tSe usa durante la tarea de preentrenamiento de modelado de lenguaje enmascarado (MLM).\n",
        "\t-\tReemplaza aleatoriamente algunos tokens en la entrada, y el modelo se entrena para predecir el token original.\n",
        "\t-\tAyuda al modelo a aprender del contexto tanto a la izquierda como a la derecha en la secuencia.\n",
        "\n",
        "El token [CLS] es un token especial que insertamos al principio de cada secuencia de entrada. Inicialmente, es simplemente un vector aprendible (una incrustación), similar a los demás tokens. Sin embargo, una vez que la secuencia pasa por todas las capas de transformadores en BERT, el estado oculto correspondiente al token [CLS] se convierte en una representación rica y contextual de toda la entrada.\n",
        "\n",
        "Intuitivamente, imaginá que tenés una discusión en grupo (la secuencia de entrada). El token [CLS] es como una persona en esa discusión que escucha a todos y luego resume lo que se dijo.\n",
        "Al principio, esta persona solo conoce su propia opinión (la incrustación inicial). Pero después de escuchar a todos (procesamiento mediante capas de self-attention), su resumen final (el estado oculto final) refleja toda la conversación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGH7p-DwGwI"
      },
      "source": [
        "### 3.2 Entrenamiento del tokenizador: ¿Cómo se entrena el Tokenizador de BERT (WordPiece)?\n",
        "\n",
        "El tokenizador de BERT utiliza el algoritmo WordPiece para dividir el texto en subunidades léxicas (subwords). Este proceso ayuda a:\n",
        "\t- Manejar palabras raras o fuera de vocabulario: Dividiendo palabras poco frecuentes en subunidades más comunes.\n",
        "\t- Reducir el tamaño del vocabulario: Un vocabulario más pequeño (típicamente de unas 30.000 unidades) cubre la mayoría de los patrones del lenguaje.\n",
        "\t-\tMantener la eficiencia: La tokenización en subpalabras equilibra entre tener demasiados tokens (nivel caracter) y muy pocos (nivel palabra), haciendo que el modelo sea robusto y eficiente.\n",
        "\n",
        "Pasos clave en el entrenamiento del tokenizador WordPiece:\n",
        "\n",
        "1.\tRecolección de datos: Se recopila un gran corpus de texto que represente el lenguaje.\n",
        "2.\tTokenización inicial: El texto se divide primero en caracteres individuales (o tokens simples) y luego se preprocesa (a menudo usando espacios en blanco).\n",
        "3.\tInicialización del vocabulario: El vocabulario inicial puede comenzar con un conjunto básico de caracteres o palabras comunes.\n",
        "4.\tUnificación iterativa (algoritmo greedy):\n",
        "  - Conteo de frecuencias: Se cuenta la frecuencia de todas las subunidades en el corpus.\n",
        "  - Decisión de combinación: Se fusionan iterativamente los pares de tokens cuya combinación resulta en el mayor aumento de verosimilitud (o frecuencia), manteniendo el tamaño del vocabulario bajo un límite especificado.\n",
        "  - Nuevas subunidades: Cada fusión crea un nuevo token (por ejemplo, combinar “in” y “feliz” podría dar “infeliz”).\n",
        "  - Repetir: El proceso continúa hasta alcanzar el tamaño de vocabulario deseado.\n",
        "5. Tokens especiales:\n",
        "El vocabulario se complementa con tokens especiales como [PAD], [UNK], [CLS], [SEP] y [MASK], que son esenciales para tareas posteriores y el entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B0rhKTLwGwJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# corpus de ejemplo: en la práctica, se usa un dataset grande (e.g., Wikipedia text)\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"BERT stands for Bidirectional Encoder Representations from Transformers.\",\n",
        "    \"Tokenization is essential for natural language processing.\",\n",
        "    \"Deep learning models require large amounts of data.\"\n",
        "]\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Entrenamiento del tokenizador.\n",
        "tokenizer.train_from_iterator(\n",
        "    texts,\n",
        "    vocab_size=3000,  # Generalmente se usa 30k tokens\n",
        "    min_frequency=1,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "save_path = \"/content/tokenizer\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "tokenizer.save_model(save_path)\n",
        "\n",
        "print(\"Vocabulary size:\", len(tokenizer.get_vocab()))\n",
        "print(\"First 10 vocabulary items:\")\n",
        "for token, index in list(tokenizer.get_vocab().items())[:10]:\n",
        "    print(f\"Token: {token}, Index: {index}\")\n",
        "\n",
        "# Tokenizar una oración como ejemplo, ¿qué observa?\n",
        "sample_text = \"BERT tokenization is fascinating!\"\n",
        "encoded = tokenizer.encode(sample_text)\n",
        "print(\"\\nOriginal Sentence:\")\n",
        "print(sample_text)\n",
        "print(\"\\nTokenized Output:\")\n",
        "print(encoded.tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Da6hCqXwGwM"
      },
      "source": [
        "### 3.3. Preentrenamiento de BERT\n",
        "---\n",
        "\n",
        "#### 3.3.1 Modelado de Lenguaje enmascarado (MLM)\n",
        "\n",
        "**Objetivo:**  \n",
        "Dada una secuencia de entrada de tokens $ \\mathbf{x} = [x_1, x_2, \\ldots, x_n] $, el objetivo del MLM es predecir un subconjunto de tokens que han sido enmascarados. Sea $ M \\subset \\{1, 2, \\ldots, n\\} $ el conjunto de posiciones enmascaradas.\n",
        "\n",
        "**Proceso:**\n",
        "\n",
        "1. **Enmascaramiento:**  \n",
        "   Para cada posición $ i \\in M $, el token original $ x_i $ se reemplaza por un token especial $[MASK]$ (o, en la práctica, a veces por un token aleatorio o se deja sin cambios, como parte de una estrategia de enmascaramiento específica).  \n",
        "   Definimos la secuencia enmascarada como:\n",
        "   $$\n",
        "   \\mathbf{\\tilde{x}} = [\\tilde{x}_1, \\tilde{x}_2, \\ldots, \\tilde{x}_n],\n",
        "   $$\n",
        "   donde:\n",
        "   $$\n",
        "   \\tilde{x_i} =\n",
        "   \\begin{cases}\n",
        "   \\text{[MASK]}, & \\text{si } i \\in M, \\\\\n",
        "   x_i, & \\text{si } i \\notin M.\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "2. **Predicción:**  \n",
        "   El modelo procesa la secuencia enmascarada $\\mathbf{\\tilde{x}}$ y produce una distribución de probabilidad sobre el vocabulario para cada posición. Sea $ p_{\\theta}(x_i \\mid \\mathbf{\\tilde{x}}) $ la probabilidad que el modelo asigna al token original $ x_i $ en la posición $ i $, dado el input enmascarado, donde $ \\theta $ representa los parámetros del modelo.\n",
        "\n",
        "3. **Función de pérdida:**  \n",
        "   La pérdida de MLM es la log-verosimilitud negativa de los tokens verdaderos en las posiciones enmascaradas:\n",
        "   $$\n",
        "   \\mathit{L}_{\\text{MLM}}(\\theta) = - \\sum_{i \\in M} \\log p_{\\theta}(x_i \\mid \\mathbf{\\tilde{x}}).\n",
        "   $$\n",
        "   Durante el entrenamiento, los parámetros del modelo $ \\theta $ se optimizan para minimizar $ \\mathit{L}_{\\text{MLM}} $.\n",
        "\n",
        "Esta formulación de log-verosimilitud negativa es matemáticamente equivalente a la pérdida de entropía cruzada en este contexto. Esta equivalencia se da porque, para cada posición enmascarada $ i $, el token verdadero se representa como un vector one-hot, lo que implica que solo la probabilidad asignada al token correcto contribuye a la pérdida.\n",
        "\n",
        "Por lo tanto, la pérdida de entropía cruzada definida como:\n",
        "$$\n",
        "-\\sum_{j} y_j \\log p_{\\theta}(x_j \\mid \\tilde{\\mathbf{x}})\n",
        "$$\n",
        "(donde $ y_j = 1 $ para el token correcto y 0 para los demás) se simplifica a:\n",
        "$$\n",
        "-\\log p_{\\theta}(x_i \\mid \\tilde{\\mathbf{x}}),\n",
        "$$\n",
        "que es exactamente el término de log-verosimilitud negativa. Así, cuando la distribución objetivo es one-hot, minimizar la log-verosimilitud negativa es equivalente a minimizar la entropía cruzada.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3.2 Predicción de la Siguiente Oración (NSP)\n",
        "\n",
        "**Objetivo:**  \n",
        "Dadas dos oraciones $ A $ y $ B $, NSP entrena al modelo para predecir si $ B $ es efectivamente la oración que sigue a $ A $ en el texto original. Sea $y$ una etiqueta binaria:\n",
        "$$\n",
        "y =\n",
        "\\begin{cases}\n",
        "1, & \\text{si } B \\text{ es la oración siguiente real después de } A, \\\\\n",
        "0, & \\text{si } B \\text{ es una oración muestreada aleatoriamente (es decir, no la real siguiente)}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Proceso:**\n",
        "\n",
        "1. **Construcción de entrada:**  \n",
        "   Las dos oraciones $ A $ y $ B $ se concatenan en una única secuencia con un token de separación especial $[SEP]$ entre ellas. Un token especial de clasificación $[CLS]$ se antepone a la secuencia. Formalmente, la entrada es:\n",
        "   $$\n",
        "   \\mathbf{x} = [\\text{[CLS]}, A, \\text{[SEP]}, B, \\text{[SEP]}].\n",
        "   $$\n",
        "   El modelo procesa esta secuencia y calcula representaciones contextualizadas para cada token.\n",
        "\n",
        "2. **Clasificación:**  \n",
        "   La representación correspondiente al token $[CLS]$, denotada como $ \\mathbf{h}_{\\text{[CLS]}} $, se utiliza para calcular una distribución de probabilidad sobre las dos clases (es decir, si $ B $ sigue a $ A $ o no). Esto se realiza con una capa de clasificación simple:\n",
        "   $$\n",
        "   p_{\\theta}(y \\mid A, B) = \\text{softmax}(W \\mathbf{h}_{\\text{[CLS]}} + b),\n",
        "   $$\n",
        "   donde $ W $ y $ b $ son parámetros aprendibles del clasificador.\n",
        "\n",
        "3. **Función de pérdida:**  \n",
        "   La pérdida de NSP es la pérdida de entropía cruzada para la clasificación binaria:\n",
        "   $$\n",
        "   \\mathit{L}_{\\text{NSP}}(\\theta) = - \\left[ y \\log p_{\\theta}(y=1 \\mid A, B) + (1-y) \\log p_{\\theta}(y=0 \\mid A, B) \\right].\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3.3 Objetivo combinado de preentrenamiento\n",
        "\n",
        "El objetivo general de preentrenamiento de BERT combina las pérdidas de MLM y NSP:\n",
        "$$\n",
        "\\mathit{L}(\\theta) = \\mathit{L}_{\\text{MLM}}(\\theta) + \\mathit{L}_{\\text{NSP}}(\\theta).\n",
        "$$\n",
        "Durante el preentrenamiento, los parámetros del modelo $ \\theta $ se actualizan para minimizar esta pérdida combinada, aprendiendo así tanto predicciones a nivel de token como relaciones a nivel de oraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqnah-sGwGwM"
      },
      "outputs": [],
      "source": [
        "# Implemente la función mask_tokens para enmascarar\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.optim import AdamW\n",
        "\n",
        "class DummyPretrainingDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=16, mask_prob=0.15):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.mask_prob = mask_prob\n",
        "    # >>>>> Comienza su implementación\n",
        "    def mask_tokens(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Implementar el enmascaramiento de tokens para la tarea de MLM (Modelado de Lenguaje Enmascarado).\n",
        "        Pasos:\n",
        "        1. Crear una copia de `input_ids` para generar las entradas enmascaradas (`masked inputs`).\n",
        "        2. Crear un tensor `labels` clonando `input_ids`. Luego, se establecerán las posiciones no enmascaradas en -100.\n",
        "        3. Generar una matriz de probabilidades (con la misma forma que `input_ids`) llena con `self.mask_prob`.\n",
        "        4. Multiplicar la matriz de probabilidades por la `attention_mask` para anular las probabilidades de los tokens de padding.\n",
        "        5. Muestrear una máscara binaria desde la matriz de probabilidades utilizando `torch.bernoulli`.\n",
        "        6. Establecer en `-100` las posiciones en `labels` que no fueron enmascaradas (es decir, donde `mask` es False).\n",
        "        7. Reemplazar los tokens en las entradas (`inputs`) por el ID del token `[MASK]` en las posiciones donde `mask` es True.\n",
        "        8. Devolver los `input_ids` enmascarados y las `labels`.\n",
        "        \"\"\"\n",
        "        pass  # TODO\n",
        "    # <<<<< Fin de su implementación\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        # Tokenize and pad/truncate the text to max_length\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()  # shape: (max_length,)\n",
        "        attention_mask = encoding['attention_mask'].squeeze()  # shape: (max_length,)\n",
        "        # Pass both input_ids and attention_mask to mask_tokens\n",
        "        input_ids_masked, labels = self.mask_tokens(input_ids, attention_mask)\n",
        "        return {\n",
        "            'input_ids': input_ids_masked,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué enmascamos?\n",
        "\n",
        "- **Aprendizaje a partir del contexto:**  \n",
        "  Al enmascarar ciertos tokens, el modelo se ve forzado a predecirlos utilizando el contexto circundante. Esto favorece el aprendizaje de representaciones del lenguaje ricas y bidireccionales.\n",
        "\n",
        "- **Evitar el ruido:**  \n",
        "  La función utiliza la máscara de atención para asegurarse de que los tokens de padding no sean seleccionados para enmascaramiento. Enmascarar tokens de relleno sería contraproducente, ya que no representan contenido real.\n",
        "\n",
        "- **Cálculo eficiente de la pérdida:**  \n",
        "  Al establecer las etiquetas de los tokens no enmascarados en `-100`, nos aseguramos de que la pérdida se calcule solo en las posiciones enmascaradas. Esto enfoca el aprendizaje del modelo en predecir correctamente los tokens ocultos."
      ],
      "metadata": {
        "id": "5QoPxiX_TBz0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on3O4xFpwGwN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertConfig, BertForMaskedLM\n",
        "\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"BERT stands for Bidirectional Encoder Representations from Transformers.\",\n",
        "    \"Pre-training is computationally expensive but beneficial.\",\n",
        "    \"Masked language modeling helps in understanding context.\"\n",
        "]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "pretrain_dataset = DummyPretrainingDataset(texts, tokenizer, max_length=16)\n",
        "pretrain_loader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# --- Muestre un ejemplo ----\n",
        "\n",
        "sample_text = texts[0]\n",
        "encoding = tokenizer(sample_text, truncation=True, padding='max_length', max_length=16, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].squeeze()  # shape: (max_length,)\n",
        "attention_mask = encoding['attention_mask'].squeeze()  # shape: (max_length,)\n",
        "\n",
        "# token_ids -> tokens\n",
        "original_tokens = tokenizer.convert_ids_to_tokens(input_ids.tolist())\n",
        "attention_mask_list = attention_mask.tolist()\n",
        "\n",
        "# Aplicar la máscara\n",
        "masked_input_ids, labels = pretrain_dataset.mask_tokens(input_ids, attention_mask)\n",
        "masked_tokens = tokenizer.convert_ids_to_tokens(masked_input_ids.tolist())\n",
        "\n",
        "label_tokens = [\"-100\" if l == -100 else tokenizer.convert_ids_to_tokens([l])[0] for l in labels.tolist()]\n",
        "\n",
        "print(\"=== Masking Demonstration for Sample Text ===\")\n",
        "print(\"Original Tokens:\")\n",
        "print(original_tokens)\n",
        "print(\"\\nAttention Mask:\")\n",
        "print(attention_mask_list)\n",
        "print(\"\\nMasked Tokens:\")\n",
        "print(masked_tokens)\n",
        "print(\"\\nLabels (only masked positions show original tokens):\")\n",
        "print(label_tokens)\n",
        "print(\"=============================================\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Pre-training Loop ---\n",
        "\n",
        "# Crear una configuración para BERT\n",
        "config = BertConfig()\n",
        "model_pretrain = BertForMaskedLM(config)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pretrain.to(device)\n",
        "optimizer = AdamW(model_pretrain.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"Starting Pre-training Loop:\")\n",
        "model_pretrain.train()\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    for batch in pretrain_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_pretrain(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        print(f\"Batch Loss: {loss.item():.4f}\")\n",
        "    avg_loss = total_loss / len(pretrain_loader)\n",
        "    print(f\"Pre-training Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Ag6Mek0pbsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GQs8Q0ewGwO"
      },
      "source": [
        "### 3.4. Benchmarks\n",
        "\n",
        "Los benchmarks son fundamentales para evaluar el rendimiento y la capacidad de generalización de los modelos de lenguaje. Para BERT y modelos similares, han surgido varios benchmarks que se han convertido en pruebas estándar para diversas tareas. A continuación, se presenta algunos ejemplos:\n",
        "\n",
        "**GLUE (General Language Understanding Evaluation):**  \n",
        "- **Descripción general:**  \n",
        "  GLUE es una colección de tareas diversas diseñadas para evaluar la comprensión general del lenguaje de un modelo. Incluye tareas como:\n",
        "  - **CoLA:** Aceptabilidad gramatical de oraciones.\n",
        "  - **SST-2:** Análisis de sentimiento.\n",
        "  - **MRPC:** Detección de paráfrasis.\n",
        "  - **STS-B:** Similitud semántica entre textos.\n",
        "  - **QQP:** Pares de preguntas de Quora para identificar paráfrasis.\n",
        "  - **MNLI, QNLI, RTE:** Tareas de inferencia de lenguaje natural.\n",
        "- GLUE evalúa la capacidad del modelo para comprender sintaxis, semántica y razonamiento a través de múltiples conjuntos de datos y tareas. Un alto rendimiento en GLUE suele considerarse un indicador clave de un modelo de lenguaje general potente.\n",
        "\n",
        "---\n",
        "\n",
        "**SuperGLUE:**  \n",
        "- **Descripción general:**  \n",
        "  Es una extensión de GLUE que introduce tareas más desafiantes, pensadas para modelos que ya han superado el rendimiento en GLUE.\n",
        "\n",
        "- Eleva el nivel de exigencia, evaluando razonamiento más sutil y comprensión más profunda del lenguaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odm7-TnCzCB8"
      },
      "outputs": [],
      "source": [
        "! pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvs2Uct2wGwO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ================================\n",
        "# GLUE - SST-2 (Sentiment Analysis)\n",
        "# ================================\n",
        "\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "print(\"\\n=== GLUE SST-2 Dataset Overview ===\")\n",
        "print(sst2_dataset)\n",
        "print(\"\\nFirst example in SST-2 Train Split:\")\n",
        "print(sst2_dataset[\"train\"][0])\n",
        "print(\"\\nKeys in the first SST-2 example:\")\n",
        "print(list(sst2_dataset[\"train\"][0].keys()))\n",
        "print(\"\\nSample Data Points in SST-2 (first 3 examples):\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(sst2_dataset[\"train\"][i])\n",
        "\n",
        "# ================================\n",
        "# GLUE - MRPC (Microsoft Research Paraphrase Corpus)\n",
        "# ================================\n",
        "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "print(\"\\n=== GLUE MRPC Dataset Overview ===\")\n",
        "print(mrpc_dataset)\n",
        "print(\"\\nFirst example in MRPC Train Split:\")\n",
        "print(mrpc_dataset[\"train\"][0])\n",
        "print(\"\\nKeys in the first MRPC example:\")\n",
        "print(list(mrpc_dataset[\"train\"][0].keys()))\n",
        "print(\"\\nSample Data Points in MRPC (first 3 examples):\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(mrpc_dataset[\"train\"][i])\n",
        "\n",
        "# ================================\n",
        "# GLUE - MNLI (Multi-Genre Natural Language Inference)\n",
        "# ================================\n",
        "mnli_dataset = load_dataset(\"glue\", \"mnli\")\n",
        "print(\"\\n=== GLUE MNLI Dataset Overview ===\")\n",
        "print(mnli_dataset)\n",
        "print(\"\\nFirst example in MNLI Train Split:\")\n",
        "print(mnli_dataset[\"train\"][0])\n",
        "print(\"\\nKeys in the first MNLI example:\")\n",
        "print(list(mnli_dataset[\"train\"][0].keys()))\n",
        "print(\"\\nSample Data Points in MNLI (first 3 examples):\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(mnli_dataset[\"train\"][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayuRZdpEwGwP"
      },
      "source": [
        "**SQuAD (Stanford Question Answering Dataset):**  \n",
        "- **Descripción general:**  \n",
        "  SQuAD es un benchmark para tareas de comprensión lectora. A los modelos se les proporciona un pasaje y una pregunta, y deben extraer el fragmento de texto (span) que contiene la respuesta dentro del pasaje.\n",
        "\n",
        "- **Métricas:**  \n",
        "  Se evalúa típicamente usando Exact Match (EM) y puntajes F1.\n",
        "\n",
        "- **Importancia:**  \n",
        "  Evalúa directamente la capacidad del modelo para localizar y comprender información relevante en un contexto dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbfs0F8wwGwP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ================================\n",
        "# SQuAD (Stanford Question Answering Dataset)\n",
        "# ================================\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "print(\"=== SQuAD Dataset Overview ===\")\n",
        "print(squad_dataset)\n",
        "print(\"\\nFirst example in SQuAD Train Split:\")\n",
        "print(squad_dataset[\"train\"][0])\n",
        "print(\"\\nKeys in the first SQuAD example:\")\n",
        "print(list(squad_dataset[\"train\"][0].keys()))\n",
        "\n",
        "print(\"\\nSample Data Points in SQuAD (first 3 examples):\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(squad_dataset[\"train\"][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpjJTc6lwGwP"
      },
      "source": [
        "**Otros Benchmarks Específicos por Tarea:**  \n",
        "- **Reconocimiento de Entidades Nombradas (NER):**  \n",
        "  Conjuntos de datos como CoNLL-2003 evalúan la capacidad del modelo para etiquetar entidades dentro del texto (personas, organizaciones, lugares, etc.).\n",
        "\n",
        "- **Traducción Automática y Resumen:**  \n",
        "  Aunque BERT no se utiliza directamente en estas tareas, adaptaciones y extensiones de BERT (como BERT2BERT o BART) son evaluadas en benchmarks como WMT (traducción) y CNN/Daily Mail (resumen).\n",
        "\n",
        "- **Comprensión Lectora:**  \n",
        "  Conjuntos de datos como HotpotQA y Natural Questions evalúan en mayor profundidad la comprensión y las capacidades de razonamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL_oCysIwGwQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ================================\n",
        "# CoNLL-2003 (Named Entity Recognition)\n",
        "# ================================\n",
        "conll_dataset = load_dataset(\"conll2003\")\n",
        "print(\"\\n=== CoNLL-2003 Dataset Overview ===\")\n",
        "print(conll_dataset)\n",
        "print(\"\\nFirst example in CoNLL-2003 Train Split:\")\n",
        "print(conll_dataset[\"train\"][0])\n",
        "print(\"\\nKeys in the first CoNLL-2003 example:\")\n",
        "print(list(conll_dataset[\"train\"][0].keys()))\n",
        "\n",
        "print(\"\\nSample Data Points in CoNLL-2003 (first 3 examples):\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(conll_dataset[\"train\"][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBJLVMs4wGwQ"
      },
      "source": [
        "### 3.5 Objetivos de Entrenamiento: Preentrenamiento vs. Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3.5.1. Objetivo de Preentrenamiento\n",
        "\n",
        "- **Meta:**  \n",
        "  Aprender representaciones lingüísticas generales entrenando el modelo para predecir información faltante en el texto.\n",
        "\n",
        "- **Tareas:**\n",
        "  - **Modelado de Lenguaje Enmascarado (MLM):** Dada una secuencia de entrada $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$, se reemplaza un subconjunto aleatorio de tokens por el token especial $[MASK]$. El modelo se entrena para predecir los tokens originales en las posiciones enmascaradas utilizando el contexto:\n",
        "  $$\n",
        "  \\mathit{L}_{\\text{MLM}}(\\theta) = -\\sum_{i \\in M} \\log p_\\theta(x_i \\mid \\tilde{\\mathbf{x}})\n",
        "  $$\n",
        "  donde $M$ es el conjunto de posiciones enmascaradas y $\\tilde{\\mathbf{x}}$ es la entrada modificada.\n",
        "\n",
        "- **Características:**\n",
        "  - **No supervisado / Auto-supervisado:**  \n",
        "    Utiliza grandes cantidades de texto sin anotar.\n",
        "  - **Aprendizaje de Representaciones Generales:**  \n",
        "    Se enfoca en aprender la estructura del lenguaje, su semántica y el contexto a través de predicciones a nivel de token (y a veces a nivel de oración).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.5.2. Objetivo de Fine-tuning\n",
        "\n",
        "- **Meta:**  \n",
        "  Adaptar las representaciones generales del lenguaje a una tarea específica (por ejemplo, clasificación de texto, preguntas y respuestas, etc.).\n",
        "\n",
        "- **Ejemplo: Clasificación de Secuencias**\n",
        "  - **Tarea:**  \n",
        "    Dado un texto de entrada, clasificarlo en una de $C$ categorías.\n",
        "\n",
        "  - **Enfoque:**  \n",
        "  El modelo se extiende con una capa específica para la tarea (generalmente una capa lineal) sobre la representación del token [CLS]:\n",
        "  $$\n",
        "  \\mathbf{h}_{\\text{[CLS]}} = \\text{BERT}(\\mathbf{x})\n",
        "  $$\n",
        "  $$\n",
        "  \\mathbf{z} = W \\mathbf{h}_{\\text{[CLS]}} + b\n",
        "  $$\n",
        "  donde $W$ y $b$ son los pesos y el sesgo de la capa de clasificación.\n",
        "\n",
        "  - **Objetivo (Pérdida de Entropía Cruzada):**\n",
        "  $$\n",
        "  {L}_{\\text{cls}}(\\theta) = -\\sum_{c=1}^{C} y_c \\log \\left( \\frac{\\exp(z_c)}{\\sum_{j=1}^{C} \\exp(z_j)} \\right)\n",
        "  $$\n",
        "  donde $y$ es la etiqueta verdadera codificada en one-hot y $z_c$ es la puntuación para la clase $c$.\n",
        "\n",
        "- **Características:**\n",
        "  - **Supervisado:**  \n",
        "    Requiere datos etiquetados específicos de la tarea.\n",
        "  - **Adaptación Específica a la Tarea:**  \n",
        "    El finetuning adapta los pesos del modelo (a menudo con un `lr` más bajo) para que las representaciones se ajusten mejor a la tarea, en lugar de representar el lenguaje de forma general.\n",
        "  - **Función de pérdida diferente:**  \n",
        "    A diferencia del preentrenamiento, que utiliza funciones de pérdida como MLM (y a veces NSP), el finetuning tiliza pérdidas acordes a la tarea (por ejemplo, entropía cruzada para clasificación).\n",
        "\n",
        "\n",
        "#### 3.5.3. Diferencias Clave\n",
        "\n",
        "- **Requisitos de Datos:**  \n",
        "  - *Preentrenamiento:* Usa texto a gran escala no etiquetado.  \n",
        "  - *Fine-tuning:* Usa datos etiquetados relevantes para la tarea específica.\n",
        "\n",
        "- **Enfoque de Aprendizaje:**  \n",
        "  - *Preentrenamiento:* Aprende representaciones generales del lenguaje mediante auto-supervisión, prediciendo tokens faltantes o continuidad entre oraciones.  \n",
        "  - *Fine-tuning:* Ajusta esas representaciones para optimizar el rendimiento en una tarea concreta mediante una señal supervisada.\n",
        "\n",
        "- **Adaptación del Modelo:**  \n",
        "  - *Preentrenamiento:* No incluye capas específicas de tarea (salvo en casos como NSP).  \n",
        "  - *Fine-tuning:* Agrega y entrena una cabeza específica para la tarea (por ejemplo, una capa de clasificación), y también puede ajustar las capas del codificador.\n",
        "\n",
        "- **Funciones de Pérdida:**  \n",
        "  - *Pérdida en Preentrenamiento:*  \n",
        "  ${L}_{\\text{pretrain}} = {L}_{\\text{MLM}} + {L}_{\\text{NSP}}$  \n",
        "  - *Pérdida en Fine-tuning:*  \n",
        "  ${L}_{\\text{task}} = {L}\n",
        "  _{\\text{cls}}$ (u otra pérdida apropiada para la tarea)"
      ],
      "metadata": {
        "id": "x6I4jrYRWAOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yqidnn2wGwQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "sst2_train = sst2[\"train\"].select(range(5000))\n",
        "sst2_val = sst2[\"validation\"].select(range(500))\n",
        "\n",
        "\n",
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=128):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        text = item[\"sentence\"]\n",
        "        label = item[\"label\"]\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding=\"max_length\",\n",
        "            max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()  # shape: (max_length,)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()  # shape: (max_length,)\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Inicializar el tokenizer\n",
        "tokenizer =  # TODO\n",
        "# Crear el training_dataset y dataloader usando el train split de SST-2\n",
        "train_dataset = # TODO\n",
        "train_loader = # TODO\n",
        "\n",
        "#  BERT-based classification model\n",
        "class CustomBertForClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(CustomBertForClassification, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    # >>>> Comienza su implementación\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        Implementar el forward pass para el modelo personalizado de clasificación con BERT.\n",
        "\n",
        "        Pasos:\n",
        "        1. Pasar los `input_ids` y `attention_mask` a través del modelo BERT.\n",
        "        2. Extraer la salida agrupada (`pooled output`) correspondiente al token [CLS] de las salidas del modelo.\n",
        "        3. Aplicar dropout a la salida agrupada.\n",
        "        4. Pasar el vector resultante por la cabeza de clasificación (una capa lineal) para obtener los logits.\n",
        "        5. Si se proporcionan etiquetas (`labels`), calcular la pérdida de entropía cruzada entre los logits y las etiquetas.\n",
        "        6. Devolver un diccionario que contenga la pérdida (si fue calculada) y los logits.\n",
        "        \"\"\"\n",
        "        pass  # Replace this with your implementation.\n",
        "    # <<<<< Fin de su implementación\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomBertForClassification(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "print(\"Starting Fine-Tuning Loop:\")\n",
        "model.train()\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"Batch Loss: {loss.item():.4f}\")\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nFine-Tuning Completed.\")\n",
        "\n",
        "# Evaluación\n",
        "print(\"\\nEvaluating on Validation Set:\")\n",
        "val_dataset = SST2Dataset(sst2_val, tokenizer, max_length=128)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "total_val_loss = 0.0\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        logits = outputs[\"logits\"]\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "avg_val_loss = total_val_loss / len(val_loader)\n",
        "accuracy = total_correct / total_samples\n",
        "\n",
        "print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "EAoe4mJ8c9-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mja6UDGwGwR"
      },
      "source": [
        "#### 3.5.4. BERT para Preguntas y Respuestas\n",
        "\n",
        "BERT resuelve tareas de comprensión lectora (QA) transformando el problema en una tarea de extracción de spans. A continuación se detalla cómo predice las posiciones de inicio y fin de una respuesta:\n",
        "\n",
        "1. **Construcción de la Entrada:**  \n",
        "   La secuencia de entrada se forma concatenando la pregunta y el contexto en una sola secuencia:  \n",
        "   ```\n",
        "   [CLS] Pregunta [SEP] Contexto [SEP]  \n",
        "   ```\n",
        "   Esto permite a BERT considerar la pregunta y el contexto de forma conjunta.\n",
        "\n",
        "2. **Codificación con BERT:**  \n",
        "   La secuencia concatenada se pasa por BERT, que genera una representación contextual para cada token.  \n",
        "   Denotamos esto como:  \n",
        "   $H = [h_1, h_2, ..., h_n],$  \n",
        "   donde cada vector $h_1 \\in\\mathbb{R}^d$ representa el token en la posición $i$.\n",
        "\n",
        "3. **Cabezas de Predicción de Span:**  \n",
        "   Se aplican dos capas lineales a los estados ocultos para calcular:\n",
        "   - Logits de inicio:  \n",
        "     `start_logits_1 = w_start^t · h1 + b_start`\n",
        "   - Logits de fin:  \n",
        "     `end_logits_1 = w_end^t · h1 + b_end`\n",
        "   Estos valores reflejan la confianza del modelo de que un token es el inicio o el fin de la respuesta.\n",
        "\n",
        "4. **Distribución de Probabilidades y Predicción:**  \n",
        "   Los logits se pasan por una función softmax para convertirlos en distribuciones de probabilidad:  \n",
        "   ```\n",
        "   p_start(i) = exp(start_logitsᵢ) / sum_j exp(start_logitsⱼ)  \n",
        "   p_end(i) = exp(end_logitsᵢ) / sum_j exp(end_logitsⱼ)  \n",
        "   ```\n",
        "   Se seleccionan los índices con mayor probabilidad como posiciones de inicio y fin de la respuesta.\n",
        "\n",
        "5. **Objetivo de Entrenamiento:**  \n",
        "   Durante el fine-tuning, el modelo se entrena minimizando la pérdida de entropía cruzada para las posiciones de inicio y fin:\n",
        "   ```  \n",
        "   L_start = -log p_start(i_true_start)  \n",
        "   L_end = -log p_end(i_true_end)  \n",
        "   ```\n",
        "   La pérdida total es:  \n",
        "   ```\n",
        "   L = L_start + L_end  \n",
        "   ```\n",
        "   Esto entrena al modelo para asignar alta probabilidad a las posiciones correctas de la respuesta dentro del contexto.\n",
        "\n",
        "---\n",
        "\n",
        "### Resumen\n",
        "\n",
        "BERT para QA no genera texto, sino que extrae una respuesta al:\n",
        "- Codificar conjuntamente la pregunta y el contexto.\n",
        "- Usar dos capas lineales para predecir los puntajes de inicio y fin.\n",
        "- Convertir esos puntajes en probabilidades y seleccionar el span con mayor confianza.\n",
        "\n",
        "Este enfoque aprovecha la fuerte comprensión contextual de BERT para localizar con precisión la respuesta dentro del contexto proporcionado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyg3HBpDwGwR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW\n",
        "\n",
        "class DummyQADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_length=384):\n",
        "        \"\"\"\n",
        "        examples: a list of dicts, each with 'question', 'context', 'answer_text', 'answer_start'\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        question = ex['question']\n",
        "        context = ex['context']\n",
        "        answer_text = ex['answer_text']\n",
        "        answer_start = ex['answer_start']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            context,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()   # shape: (max_length,)\n",
        "        attention_mask = encoding['attention_mask'].squeeze()  # shape: (max_length,)\n",
        "        offsets = encoding['offset_mapping'].squeeze() # shape: (max_length, 2)\n",
        "\n",
        "        # Encontrar las posiciones de inicio y fin dentro del contexto tokenizado\n",
        "        # (En un caso real, se debe usar un método más robusto; esto es solo para demostración.)\n",
        "        # Supondremos que la respuesta está completamente contenida en la parte del contexto.\n",
        "        start_pos, end_pos = 0, 0\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start <= answer_start < end:\n",
        "                start_pos = idx\n",
        "            if start < answer_start + len(answer_text) <= end:\n",
        "                end_pos = idx\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'start_positions': torch.tensor(start_pos, dtype=torch.long),\n",
        "            'end_positions': torch.tensor(end_pos, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"What does BERT stand for?\",\n",
        "        \"context\": \"BERT stands for Bidirectional Encoder Representations from Transformers. It is a powerful language model.\",\n",
        "        \"answer_text\": \"Bidirectional Encoder Representations from Transformers\",\n",
        "        \"answer_start\": 13  # starting character index of answer in the context\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is BERT?\",\n",
        "        \"context\": \"BERT is a model that has transformed natural language processing. It is pre-trained on large corpora.\",\n",
        "        \"answer_text\": \"a model that has transformed natural language processing\",\n",
        "        \"answer_start\": 7\n",
        "    }\n",
        "]\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "qa_dataset = DummyQADataset(examples, tokenizer, max_length=128)\n",
        "qa_loader = DataLoader(qa_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "model_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_qa.to(device)\n",
        "\n",
        "optimizer = AdamW(model_qa.parameters(), lr=3e-5)\n",
        "\n",
        "# Fine-tuning loop for QA\n",
        "model_qa.train()\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    for batch in qa_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "        outputs = model_qa(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            start_positions=start_positions,\n",
        "            end_positions=end_positions\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"Batch Loss: {loss.item():.4f}\")\n",
        "    avg_loss = total_loss / len(qa_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nFine-Tuning for QA Completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "NQENxjl9Yx8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Apéndice\n",
        "\n",
        "### I. Referencias:\n",
        "\n",
        "- implementación oficial de ELMo: https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py\n",
        "\n",
        "- blog con explicación de ELMo y BERT: https://jalammar.github.io/illustrated-bert/"
      ],
      "metadata": {
        "id": "nbCKHJeGl4Kk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "HWzDkdQWndwp"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}