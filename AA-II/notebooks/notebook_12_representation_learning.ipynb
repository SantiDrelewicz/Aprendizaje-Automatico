{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "pdLZjikntGv2",
        "oVfshv4AfrEk",
        "LYTB2IH3msNi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiDrelewicz/Aprendizaje-Automatico/blob/main/AA-II/notebooks/notebook_12_representation_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Representation Learning\n",
        "\n",
        "Esta actividad presenta tres enfoques clave para el aprendizaje de representaciones, desde distintas perspectivas del modelado generativo y del procesamiento del lenguaje natural:\n",
        "- Autoencoders: redes que aprenden a comprimir y reconstruir datos, forzando al modelo a capturar información esencial en un espacio latente de menor dimensión.\n",
        "- GANs: modelos que aprenden a generar datos realistas mediante una competencia entre un generador y un discriminador.\n",
        "- Word Embeddings: vectores densos que representan palabras en un espacio continuo, capturando relaciones semánticas a partir del contexto en grandes corpus no etiquetados.\n",
        "\n",
        "En todos los casos, el objetivo es aprender representaciones internas útiles mediante tareas generativas o auto-supervisadas, como la reconstrucción, la generación de ejemplos nuevos o la predicción de contexto.\n",
        "\n",
        "⸻\n",
        "\n",
        "Los objetivos son:\n",
        "- Entrenar un autoencoder simple y visualizar su espacio latente\n",
        "- Usar la representación latente aprendida para generar ejemplos nuevos\n",
        "- Entrenar una GAN básica y observar cómo aprende a generar muestras realistas\n",
        "-  Comparar distintas variantes de modelos de clasificación utilizando word embeddings:\n",
        "\t- Inicializados aleatoriamente (rand)\n",
        "\t- Preentrenados fijos (static)\n",
        "\t- Preentrenados ajustables (non-static)\n",
        "- Analizar cómo el uso de representaciones distribuidas impacta en el rendimiento de modelos supervisados de texto.\n",
        "\n"
      ],
      "metadata": {
        "id": "8JS-alPNX8XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Autoencoder\n",
        "\n",
        "### 1.1. Repaso teórico\n",
        "\n",
        "Los autoencoders son modelos que permiten apredener representaciones latentes de datos sin etiquetas. Vamos a considerar que los autoenconders son redes cuyo objetivo es aprender la función identidad, de esta forma, durante el procedimiento de entrenamiento se espera que puedan aprender a comprimir los datos de entrada.\n",
        "Matemáticamente, sea $\\boldsymbol{x}\\in {D}$, llamemos `Encoder` a $g_\\varphi(\\cdot)$ y `Decoder` a $f_\\theta(\\cdot).$ Consideremos $\\boldsymbol{z}=g_\\varphi(\\boldsymbol{x})$, generalmente de dimensión menor al input $\\boldsymbol{x}$, y sea $\\boldsymbol{x}\\prime=f_\\theta(g_\\varphi(\\boldsymbol{x})).$ El obejtivo es aprender los parámetros $\\varphi$ y $\\theta$ tales que: $x\\approx x\\prime$. Es decir:\n",
        "\n",
        "$$\n",
        "{L}(\\varphi,\\theta)=\\mathbb{E_{x}}[\\|x - f_\\theta(g_\\varphi(\\boldsymbol{x}))\\|_2^2]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "pdLZjikntGv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Ejercicios de autoencoder\n",
        "\n",
        "Para los siguientes ejercicios, vamos a usar el dataset de Fashion-MNIST. Vamos a comparar los siguientes modelos:\n",
        "1. PCA (usando sklearn)\n",
        "2. Autoencoder Lineal sin función de activación\n",
        "3. DeepAutoencoder\n",
        "4. DenoisingAutoencoder\n"
      ],
      "metadata": {
        "id": "z5TLH3GwYKLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importamos librerías\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Librerías Importadas\")"
      ],
      "metadata": {
        "id": "IGAwfAg0s6P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIONES\n",
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "aTfnKu6cMCRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos la transformación para preprocesar los datos\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # flatten 28×28 → 784\n",
        "])\n",
        "\n",
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = BATCH_SIZE\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(f\"Nros. de Train: {len(train_dataset)}\")\n",
        "print(f\"Nros. de Test: {len(test_dataset)}\")\n",
        "print(f\"Shape de la imagenes: {train_dataset[0][0].shape}\")\n",
        "print(f\"# de clases: {len(class_names)}\")"
      ],
      "metadata": {
        "id": "rsmrHLd1tksn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Veamos algunos ejemplos para cada clase\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('Fashion-MNIST', fontsize=16)\n",
        "\n",
        "\n",
        "class_samples = {}\n",
        "for data, target in train_loader:\n",
        "    for i in range(len(target)):\n",
        "        label = target[i].item()\n",
        "        if label not in class_samples:\n",
        "            class_samples[label] = data[i]\n",
        "        if len(class_samples) == 10:\n",
        "            break\n",
        "    if len(class_samples) == 10:\n",
        "        break\n",
        "\n",
        "\n",
        "for idx, (class_idx, image) in enumerate(class_samples.items()):\n",
        "    row, col = idx // 5, idx % 5\n",
        "    image = image.view(28, 28)\n",
        "    axes[row, col].imshow(image.squeeze(), cmap='gray')\n",
        "    axes[row, col].set_title(f'{class_names[class_idx]}')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "cellView": "form",
        "id": "W2W-xh4utqSC",
        "outputId": "27c04e6d-6846-474c-98b6-8056da862f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABa4AAAJRCAYAAACtACBJAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAkbtJREFUeJzs/Xd4VVX6//+/TnonhADSQTBBEqqIIIgKdkaxgNixooNtRlHRsY1vEVTsOqLY0eGrqCgqNlRQlAEFEUGK9BAg1PSenN8f/sjHCO57aQ5kg8/HdXldkvXKvddp66x9Z+ckEAwGgwIAAAAAAAAAwCfC6nsCAAAAAAAAAAD8Go1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAADsN+bOnav09HRdeeWVzt8zYMAApaena8OGDXtxZgAAAABCKaK+JwAAAAD/2rBhgwYOHOiU7dWrlyZNmrSXZ/THXXXVVSooKFBycnJ9T6XGE088oSeffFKSdP/99+v000/3zD/yyCOaMGGCJGns2LE688wza8YGDBig7OxsderUSW+++abCw8P3WGPu3Lm66KKLdM011+jaa6+t+fro0aM1depU3XHHHbrgggtqfU91dbU++OADffDBB1qzZo22bdum0tJSJSQkqE2bNjrmmGN04YUXKjExUdIfe77s8tv5AAAAABKNawAAADiIjY01m4vNmjXbR7P5Y84+++z6nsLvCgQCeuuttzwb19XV1Xr33XcVCAQUDAZ/N/fTTz/phRde0BVXXBGSuZWXl+uqq67S119/rdTUVB1zzDFq2bKlqqqqtHHjRs2cOVOPPfaY3nzzTU2ePFlNmzZVcnKybr755t1qPfPMM8rLy9Oll16q1NTUWmPdu3cPyXwBAABwYKFxDQAAAFN0dLQuu+yy+p7GAadz58769ttvlZWVpVatWu0xM2fOHG3atEmZmZlavHjxHjMpKSmKjIzUk08+qRNPPFGtW7eu89xeffVVff311+revbteeOEFxcXF1RovLy/Xtddeq5kzZ+qRRx7RuHHjlJCQsMfnyWuvvaa8vDydccYZSktLq/PcAAAAcODjM64BAAAQcqtXr9bNN9+sY445RpmZmeratatOO+00TZw4URUVFbvlv//+e1199dXq16+fMjIydMQRR2jIkCF6+eWXVVVVtcdjlJSU6L777lP//v2VmZmpY445Rg888IDKy8tr5X7vM66zsrJ0++23a8CAAcrMzNRhhx2moUOHatKkSaqsrKyVHT16tNLT0/XFF1/o+++/18UXX6zDDz9cnTt31llnnaUZM2b8qfvpuOOOUzAY1FtvvfW7mbfffltxcXHq06fP72ZiY2N15513qrS0VHfeeeefmstvzZs3T5J07rnn7ta0lqSoqCjdcccdGjZsGFdNAwAAIOS44hoAAAAhtXz5cp177rkqLS3VySefrA4dOigvL0/vvfeexo8fr59++kmPPPJITf67777T8OHDFRMTo5NPPlktWrRQQUGBZs2apfvuu08//PCDHn744VrHqK6u1ogRIxQMBnXuuecqPz9fU6ZM0fPPP6/y8nLdfvvtnnP86aefNHz4cBUUFOiYY47R4MGDVVhYqC+//FL33nuv5syZo6eeekqBQKDW9y1cuFCvvPKKTjnlFPXq1Us//fSTPv30U1177bV644031Llz5z90X3Xp0kXNmjXT1KlTdd111yksrPZ1JQUFBZoxY4YGDhyo6Ohoz1rHHXecTjzxRH388cd68803NWTIkD80l9+KjY2VJM8/atmyZUvdc889dToOAAAAsCc0rgEAABBSL7zwgoqKinTttdfqmmuuqfn68OHDddJJJ2n69Om6+uqr1aFDB0nS5MmTVVlZqYcfflhHH310Tf6f//ynhg8fri+//FKbNm2q9Rnac+bM0dChQ3XnnXfWNJePO+44nXfeeZo6darZuL7tttuUn5+vu+66S+edd17N12+44Qadc845+uyzzzR9+nQNGjSo1vc999xzmjhxoo488siar919992aPHmypk2b9ocb12FhYTrzzDP11FNPafbs2erfv3+t8enTp6u0tFSDBw/WokWLzHp33HGH5syZowceeEDHHHPMbp8n/UcMHDhQ06dP15NPPqmioiKde+65v/txJgAAAECo8VEhAAAAMAWDQW3YsMHzv/z8fEnSZZddpgkTJtRqCEu//PHGjIwMSb9clb1LXl6eJCk8PLxWPjIyUq+88oq+++673f7wY3h4uG688cZaV0Qfdthhio+PV2FhYc1c9mTZsmVaunSpmjdvrnPPPbfWWGxsrC699FJJ0nvvvbfb9x555JG1mtaS1K9fP0nS2rVrf/eYXs4666yaP9L4W2+//baaNm2qo446yqlW48aNdfPNNysvL6/OV0L/7W9/02WXXaZgMKjnn39exx13nE444QTddNNNmjx5stasWVOn+gAAAIAXrrgGAACAKS8vTwMHDvTMXHPNNbr22muVlpZW8wf4ysvLtXPnzprPtd71WcllZWU133fsscfqq6++0g033KDLLrtMxx13nNq3by9JiojY83a1TZs2SkhI2O3rSUlJKioqUkFBgZKSkvb4vT/++KMkqXv37rt9FIj0y8d3SL98nMhvZWZm7va1XfMoLS2t+VpOTk6tf+/KNWrUaLfvb9GihY488kh99tln2rlzpxo2bCjpl88JX7hwoa666qrdPkLEy5AhQzRt2jR9/PHH+uyzz8zHzcvNN9+swYMH64033tCsWbO0bt06rVu3TtOmTZMkdejQQSNGjNDgwYP/9DEAAACAPaFxDQAAAFN8fLweeOABz0y7du0k/dKU/s9//qNp06Zp48aNZu3zzz9fRUVFmjBhgh5++GE9/PDDaty4sfr166czzjhDRxxxxG7f06BBgz3W2tXgDQaDv3u8HTt2SJJSUlL2OL7r6zt37txtbE+N5z01v0eNGlXzxw13OeOMMzRu3Lg9HnPIkCH6+uuvNW3aNA0fPlzSL1dbBwIBnXXWWb97W/YkEAjo//7v/3Taaafp3//+t4444og9Nvldpaen64477tAdd9yhLVu2aMGCBfruu+/0+eefa+XKlbr55pu1fPly3XzzzX/6GAAAAMBv0bgGAACAKTIyUscdd5xTduTIkZo9e7aaN2+uq6++Wm3btq250vq5557T999/v9v3jBgxQueee65mzpyp2bNn6+uvv9bUqVM1depUDR06VPfee2/IbsueGs2/Vl1d7ZTzcv311ys3N7fW1377cSe/dtxxxyk5OVlvv/22hg8frqqqKr377rs6/PDD1bp16z98/LZt2+rqq6/Www8/rAceeCBkf0CxSZMmOumkk3TSSSfp9ttv1zvvvKNbb71VL774Ip+BDQAAgJCicQ0AAICQWbRokWbPnq2UlBRNmTJltz8O+Oqrr/7u9yYmJurUU0/VqaeeqmAwqK+//lq33XabpkyZokGDBqlPnz4hmeOuK6q3b9++x/FdV2Tv6epqVz179vxD+aioKJ122ml65ZVXtGTJEu3YsUNbtmzRjTfe+KfncNlll2n69Ol64403dNppp/2pGsFg0LOBf/rpp+utt97SvHnztHTpUhrXAAAACBn+OCMAAABCJisrS5LUtWvX3ZrWhYWFWrRo0W7fk5eXp+zs7FpfCwQC6tevny6//HJJ0pIlS0I2x86dO0uSFixYUHN19a8tWLCgVm5fGTp0qCTp448/1vvvv6+EhASdeOKJf7peRESE7r33XoWFhen2229XeXm58/fOnTtXgwYN2u2PV+5JVVWVJCkmJuZPzxUAAAD4LRrXAAAACJmmTZtK+uUPC1ZWVtZ8vaCgQKNGjar5yJD8/HxJv3yO9JFHHqlLLrlEeXl5u9Xb1bD2+piNPyo9PV1dunTR5s2bNXny5FpjhYWFeuGFFyTpD3+2dF2lpaWpS5cumj17tmbOnKlBgwYpNja2TjU7d+6sCy+8UGvWrNGECROcv69jx47avn27vv/+ez300EO1Hstfe//997VgwQKlpqaqV69edZorAAAA8Gt8VAgAAABCpmvXrurQoYNWrlypiy66SP369VNeXp4++ugj9ezZU5dddpnGjRunV155RcXFxbrooov097//XU888YQGDRqk4447TgcddJBKSkq0YMECzZs3TxkZGTr++ONDOs8xY8bowgsv1L333qs5c+aoY8eO2rlzp7744gtlZ2dr2LBhOvroo0N6TBdDhw7VHXfcISl0jfPrr79en376qb777jvn72nQoIGefPJJXXPNNXr22Wc1depUDRgwQM2aNVMwGNSOHTs0Z84crVy5UsnJyXrssce44hoAAAAhReMaAAAAIRMZGamJEyfq/vvv1/z58/Xss8+qVatWuvjiizV8+HAVFhbqyy+/1Pz58/Xmm2/qnHPO0TXXXKP09HS98cYbmjFjhnJzcxUZGam2bdvquuuu0/DhwxUVFRXSeaalpemtt97S008/rW+++UYzZ85UdHS0Dj30UF1//fUaPHhwSI/n6pRTTtHYsWPVokULde3aNSQ14+LidM899+iyyy77Q9/Xs2dPffjhh5oyZYq++uorzZgxo+ZK+aSkJLVv31433nijzj77bCUnJ4dkrgAAAMAugWAwGKzvSQAAAAAAAAAAsAufcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPCViPqeAP6chQsX6uWXX9bChQu1detWRUZGqn379ho6dKiGDRu2T+bwz3/+Uz/88IM+//xz5+954okn9OSTT2rRokWKjo7ei7MDsK/4YT36rfT0dF1xxRUaNWpUvRwfwP5v9uzZevXVV/XDDz+ooKBAqamp6tq1qy688EL17NmzvqcH4C9i9OjRmjp1as2/IyMjlZqaqm7dumnYsGHq06dPPc4OAH7fb9evsLAwpaSkqFOnTrr00ktZv+CEK673Q3PnztV5552nsLAwPfbYY/r00081adIkde3aVXfeeadefPHF+p4igL8I1iMAB6JHH31UI0aMUKtWrfTMM8/oo48+0pgxY1RcXKwLL7xQr7/+esiPecstt+iJJ54IeV0A+7+UlBTNnj1bs2fP1scff6yxY8cqPj5el156qcaNG1ff0wOA3/Xr9WvmzJl66qmnJEmXXnqpFi9eXM+zw/6AK673Q5MnT1bTpk01fvx4BQIBSVKzZs2UmZmp0tJSLVmypJ5nCOCvgvUIwIFm1qxZevrpp3XnnXfq/PPPr/l6y5YtdeSRR+r666/X+PHjddJJJ6lBgwYhO+7333+vli1bhqwegANHWFiYGjduXPPvFi1aqE+fPurTp49uvPFGdejQQUOGDKnHGQLAnv12/WratKnGjh2rvn37aubMmcrMzKzH2WF/wBXX+6HS0lJVVVWpoqJit7ExY8Zo/PjxkqSioiLde++9Ouqoo5SRkaH+/fvrtttu086dO2vyTzzxhHr27Knly5frvPPOU7du3XTMMcfo2WefrVV3/vz5OuOMM9S5c2cNGDBAr7766m7HdjkegAOL63o0YMAAjRkzRq+99poGDhyobt26aciQIVq0aFGt7/nyyy91wQUXqFevXurRo4euuOIKrVq1arfMueeeq27duql79+4644wz9Mknn3jOc/369TryyCM1atQoBYNBp2O9/fbbSk9P16xZszRw4ECdddZZf+o+ArB/eeGFF9S2bVudd955u40FAgHdc889+uyzz9SgQQMFg0E999xzOvHEE5WZmalevXrp2muv1bp162p937Rp02r2UYcddpjOPfdczZs3r2Y8PT1d69at05NPPqn09HRt2LBhr99OAPu/v/3tb+rTp0/NuduAAQN077336tZbb1XXrl1rPtJx69atuvnmmzVgwAB17txZgwYN0ptvvlmr1qeffqqzzjpLPXr0UI8ePXTOOefom2++qRlftmyZrrjiCvXu3VtdunTRKaecokmTJu27GwvggJOcnCxJqqys1GOPPaaBAwcqIyNDffv21XXXXbfbfmjGjBk6+eST1blzZ/3tb3/TrFmzdNlll+nCCy+sh9ljX6FxvR/q37+/cnJydP755+vjjz9WQUHBHnP33nuv3nvvPY0bN04zZszQQw89pLlz5+rOO++slausrNS9996rq6++WtOmTdNRRx2lhx56SAsXLpQk5ebm6qqrrlJ0dLT+v//v/9NTTz2lefPm6dtvv/1TxwNw4HBdjyTpq6++0g8//KAJEybolVdeUV5enm6++eaa8Xnz5unKK69UkyZN9N///lcvv/yyysvLdcEFF2jHjh2SfmlAjxw5UgcffLDeeecdvfvuu+rXr5/+8Y9/6KefftrjcXfs2KHLL79cmZmZGjdunAKBgNOxdnnmmWd03333acKECSG4xwD4WWVlpRYsWKCjjz665rdIfis5OVlJSUmSpMcff1yPPvqozjvvPL3//vv6z3/+o3Xr1mn48OEqKiqSJH377be66aabdPTRR2v69OmaMmWK2rZtqyuvvFI5OTmSVNNcuvTSSzV79mw1a9ZsH9xaAAeCgQMHat26ddq4caOkX35rJD4+Xu+995569+6t8vJyDR8+XPPnz9fdd9+t9957T4MHD9btt9+ud955R5K0Zs0a/eMf/9CJJ56od999V1OmTFFmZqZGjBihTZs2SZKuuuoqJSQkaNKkSZo+fbouvvhi3X///Zo+fXp93XQA+6Ft27Zp7Nixatq0qU455RRJ0oQJEzRx4kTddNNNmjFjhp5++mllZ2fruuuuq/m+n3/+Wddff71at26tKVOm6Pbbb9dDDz2020VOOAAFsd+prq4OPvHEE8EuXboE09LSgh07dgyeccYZwYceeii4evXqmlxOTk5w/fr1tb73wQcfDHbr1i1YXV0dDAaDwccffzyYlpYW/Pzzz2sy2dnZwbS0tODLL78cDAaDwTfeeCOYlpYWXLlyZU2mrKws2KtXr+Cxxx77p45XWloaonsDQH1yXY+OPfbYYN++fYNlZWU1X3viiSeCaWlpwYKCgmAwGAxedtllwYEDBwYrKytrMlu3bg1mZmYGn3766WAwGAyWlpYGV65cGSwqKqrJlJaWBtPS0oITJ06s+VpaWlrwwQcfDBYXFweHDh0aPOecc4LFxcU14y7Heuutt4JpaWnBSZMmheruAuBzW7ZsCaalpQVfeuklM1tWVhbs3r178K677qr19R9//DGYlpYWfOedd4LBYDBYVFQUXLFiRbCioqIms3LlymBaWlpw+vTpwWDw/61jjz/+eOhuDIADwi233BI88sgjf3f8008/DaalpQUXLlwYPPbYY4N9+vQJVlVV1Yx/8MEHwbS0tOCcOXNqfd/f//734AknnFArs3Xr1prxysrK4IIFC4KFhYXBbdu2BdPS0oIffPBBrRpLliwJbtmyJRQ3E8AB6JZbbgmmp6cHu3XrFuzWrVuwc+fOwbS0tOCxxx4b/O6772py27dvD65atarW9/73v/8NpqWlBbdv3x4MBoPBhx9+ONipU6dgbm5uTWbZsmXBtLS04AUXXLBvbhDqBZ9xvR8KBAK65pprNHz4cH355ZeaN2+e5s2bp2eeeUYTJ07U7bffrvPPP19hYWGaNGmSvvzyS23btq3m1/krKipUXl6u6Ojomppdu3at+f+UlBRJUn5+viRpxYoVio2NVfv27WsyUVFRyszM1Jo1a2q+9keOB+DA4LoeSVJGRoaioqJqvnfXWpOXl6eEhAQtWrRIJ5xwgsLDw2syqampOuSQQ2qupo6OjtbKlSt1zz33aNWqVTVXNEq//HbIr1VVVemGG25QYWGhJk+erNjY2Joxl2PtwueuAX8du66yDv7/P1LIy+rVq1VUVKSePXvW+nqnTp0UHR2tn376SYMHD1ZcXJwWLlyoO+64Q+vXr1dJSUlN/d+uWwDwR1VWVkpSzZ7m0EMPVVjY//vF6h9++EGRkZHq1atXre/r06ePPvvsMxUVFalHjx5KSUnRBRdcoGHDhqlPnz7q2LGjunfvLkmKi4tT9+7ddffdd2vZsmXq16+funfvrk6dOu2jWwlgf5WcnFzrj1rv3LlTs2fP1qWXXqpbbrlF5513nqKjozVt2jR99tlnysnJUUVFRc3atnPnTqWkpGj9+vVq3bp1rb8vkp6erubNm+/z24R9i8b1fiwxMVGDBg3SoEGDJElLlizRTTfdpLFjx+rEE0/UZZddpk2bNmn06NHKzMxUdHS0Jk2atMfPIouPj6/5/9+etBUVFdVq+Ozpe4LB4B86HoADi9d6dNJJJ0n65aTn13671hQWFuqdd97RBx98UCtXVlZW0/D+9NNPdd111+mkk07So48+qtTUVAUCAZ1wwgm7zemNN95QcXGxUlJSdvsMbpdj/fq2AfhraNiwoWJjY3f7jOo9KSwslLT7GhEWFqa4uLiaH6y99NJLGjt2rM4991zddtttatCggXJycvg8RgAhsW7dOgUCgZrmza6PMtqlsLBQFRUVOuyww2p9fVdTaOvWrWrbtq2mTJmi559/Xi+99JLGjRunFi1a6O9//7uGDh2qQCCg559/Xq+88oo+/PBDPfPMM0pMTNTQoUP1z3/+c7e9EwDsEh4erjZt2tT8u02bNurWrZsqKio0btw4DRo0SKNHj9bs2bM1atQoHXHEEYqNjdUnn3xS8/eSpF9+2P/rHtQuDRs23Ce3A/WHxvV+qKysTJJ2u4I5IyNDN9xwg66++mqtXr1ay5Yt07///W+deeaZNZny8vI/fLy4uDiVlpbu9vVff5btihUrQnY8APsP1/XIRVJSkvr166drr712t7FdJ0TTpk1T06ZN9cgjj9RcTbRly5Y91mvVqpUeeughXXbZZbr55pv1/PPP1zTLXY4F4K8nPDxchx9+uD7//HP961//UkTE7lvlvLw8ffzxx+rWrZsk7fbZ/tXV1SoqKqppaE+bNk3dunXT3XffXZP57WfpA8Cf9fHHHysjI6PmN9l+KykpSTExMTWfZ/1buz5Tv2XLlrrrrrt011136eeff9akSZN0++23q2XLlurTp4/i4+P197//XX//+9+1ZcsWvffee3rssccUExOj66+/fm/dPAAHqM6dO6usrEwrVqzQF198oSuuuELDhw+vGa+urq6Vj4qK2mNf6vca2jhw8McZ9zNbtmxRz5499fTTT+9xfNdfXd11ZeOvNzCFhYX65JNPJLn9CuwuBx98sIqLi/Xzzz/XfK20tFSLFy+u+feuqxlDcTwA+wfX9ahp06ZO9bp166ZVq1apTZs2tf6rrKxU48aNJf2y1jRo0KDWr8BOnTpV0u7rTL9+/dS+fXuNHz9e//vf/zRx4sQ/dCwAf02XXnqpNm/erP/85z+7jQWDQd1zzz0aO3asEhMTlZiYuNsfq168eLHKy8vVuXNnSb+sW7+9Guj31i32SwD+iEmTJmnJkiW66qqrfjfTrVs3lZaWqqSkpNaeJyYmRklJSYqKitLSpUs1Z86cmu855JBDdM899yghIUHLli1TTk5OrT/C2KRJE1122WXq27evli5duldvI4AD066Lm5o3b65gMFirl1RVVaVp06bVyrdp00Zr165VXl5ezdcWL16s7OzsfTNh1Bsa1/uZJk2a6Pzzz9eECRM0duxYLVy4UNnZ2Vq2bJkmTpyoRx55RIMHD9bBBx+sBg0a6LXXXtOaNWu0cOFCXX755TruuOMkSXPnzlVJSYnTMU844QTFxcXpnnvu0dKlS7V06VLdeOONtX7tP5THA7B/cF2PWrdu7VTv8ssv1/Lly2s+P3Ht2rV69tlndeqpp2rWrFmSfjn5WrlypaZPn66srCw9//zz+uGHH9SsWTP99NNPe7z6umfPnrrqqqv02GOP6YcffnA+FoC/pj59+ujaa6/VU089pVtuuUULFixQdna25s6dqxEjRujTTz/Vgw8+qGbNmumSSy7RW2+9pddee01ZWVmaM2eORo8erYMPPrhmD9StWzfNnTtX33zzjdatW6cHH3xQ1dXVCg8P16JFi7Rjxw5FRUUpJiZGCxcu1LJly2r+zggASL9cebh161Zt3bpVOTk5+v7773XHHXdozJgxuvLKK3X88cf/7vcee+yxSktL06hRo/TNN98oOztbs2bN0gUXXKA77rhDkrRw4UKNHDlSb731lrKyspSVlaUXXnhBxcXFOuyww5Sfn68bb7xRDz30kFauXKlNmzZpxowZWrBgwW6fnQ0Av/br9Wvr1q1au3atXn/9df3nP//R+eefrxYtWqht27Z6++23tXz5ci1dulR///vfaz7e6Ntvv1VhYaFOPvlkVVRU6J577tHKlSs1b9483XXXXWrRokU930LsbXxUyH5o9OjRysjI0JtvvqkPPvhAO3fuVExMjA455BDdcsstGjZsmMLDwzV+/HiNHTtWgwcPVps2bfSPf/xD3bt31/fff6/rrrtuj1cS7UlqaqqeeuopjR07VkOHDlXjxo116aWXqlGjRpo9e7akX67wDtXxAOw/XNYjVz179tRzzz2nJ554QsOGDVN1dbXS09P1yCOPaODAgZKkiy66SKtXr9Zdd92lQCCgY489Vg888ICmTJmiRx99VKNGjdIrr7yyW+2rr75a33zzjW644Qa98847TscC8Nd1zTXX6LDDDtPLL7+skSNHqqioSE2aNFGvXr309ttvq0OHDpKkkSNHKjo6Wi+//LLuu+8+JSYm6qijjtJNN91U87FD//jHP7R161Zdc801io6O1mmnnaa77rpLcXFxmjx5sgKBgMaOHauRI0dqwoQJOv/88/Xcc8/V/FE0ANixY4f69esn6Ze/EdKgQQN17dpVzz33XM3Xf09UVJReeukljR8/XjfeeKPy8vKUmpqqQYMG6brrrpMknXvuuSopKdFzzz2ne+65R5GRkerQoYMee+wxdenSRZI0YcIEPf3003rttddUVVWlFi1a6NJLL9XFF1+8V287gP3br9cv6Ze/lda6dWuNGjVK5557riTpwQcf1N13362hQ4eqadOmGjFihAYPHqyff/5Z9957ryIiIjRkyBDde++9evrpp3XmmWfqkEMO0a233qqxY8fyUY8HuECQ30kEAAAAAAAA4FM7duxQYmKiIiMjJf3yR2b79u2rU045RXfddVc9zw57C1dcAwAAAAAAAPClVatW6bTTTtNpp52myy+/XJL08ssvKz8/X0OGDKnn2WFv4oprAAAAAAAAAL711Vdf6amnntKKFSsUFhamDh06aOTIkerfv399Tw17EY1rAAAAAAAAAICvhNX3BAAAAAAAAAAA+DUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPCVCNdgIBDYm/MAUE+CwWB9TyEkWKOAAxNrFFxcfvnlZuaUU04xM7GxsWamqKjIczw6OtqsUVBQYGYKCwvNzKpVq8zMZ599Zma+++47M4M9Y43CvpSenm5mli9fvg9mInXp0sXM/PTTT2amsrKyznNxef66vFZDVcdP9rf5/h7WKODA5LJGccU1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXAsFgMOgUDAT29lwA1APHJcD3WKOAAxNr1P4tLMy+RqK6utpzvG/fvmaNV1991cwsXrzYzGRmZpqZwsJCz/GioiKzRrt27cxMVlaWmSkoKDAz7du3r/N8qqqqzBp/VaxRSEpKMjPJyclmZtq0aWama9euZmbKlCme4+Hh4WYNl8xRRx0VkjoZGRlmJjs728yEgsvrYH97ze9v8/09rFHAgclljeKKawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4SiAYDAadgoHA3p4LgHrguAT4HmsUcGBijcLjjz9uZnr37m1mSktLzUxlZaWZycjI8Bxv0qSJWWPDhg1m5r333jMzDRo0MDPNmjUzM19++aXn+N13323W+KtijfKvsDD7Gq3q6mozc9BBB3mOjx492qyxadMmM+PyXDr++OPNzCGHHOI5npeXZ9ZISUkxM2vXrjUzn3zyiZkpLCw0M9a6+tJLL5k1li9fbmYORKxRAPzMZY3iimsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK8EgsFg0CkYCOztuQCoB45LgO+xRgEHJtaoA9+ECRM8xxs0aGDWcHmetGrVyswUFBSYmRUrVniODxw40KyxfPlyMxMfH29mSktLzUxOTo6ZiYuL8xyvrKw0a1x66aVm5kDEGnXgu+SSSzzHDz74YLPG6tWrzYzL6ywrK8vMZGRkeI6npaWZNTZt2mRmvv32WzPTpEkTM+Oyxjds2NBzPDo62qxx9913m5kDEWsUAD9zWaO44hoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvhJR3xMAAADAgal///5m5qCDDvIcX7t2rVkjEAiYmfXr15uZ8vJyMxMXF+c5npOTY9aIjo42M8uXLzcz69atMzMHH3ywmcnOzvYcj4mJMWtYj6Mkbd682cwAftOoUSPP8e3bt5s1rHVDksrKysxMRkaGmcnNzfUc/+KLL8waLq/5tLQ0M+MiGAyamfz8fM9xl/UnPj7ezBQVFZkZ+JfLXsAlY3F5zobKvjwW6lfTpk3NzMknn2xmKioqzEx6erqZ+fjjjz3Hv/76a7NGqHDFNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8JaK+JwAAAIADU6dOncxMVFSU53hlZaVZIyEhwXlOXmJjY81Mw4YNPcdzc3PNGo0bNzYzOTk5ZiYlJcXMFBUVmZmKigrP8ejoaLNGWlqamdm8ebOZAfal1NRUM2O9zrKysswaYWH29WJVVVUhyVjrYXx8vFnDRWlpqZmJiYkxM8Fg0MxY7xPWGiZJrVq1MjPLli0zM/Avl+eSSwZ7dtJJJ5mZFi1amJny8nLP8erqarNGZGSkmXFZfxo1amRmXNa6hx56yHO8ZcuWZo2JEyeamfXr15uZGTNmmJk77rjDzFx00UWe4/n5+WaNE0880cy44IprAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPhKRH1PAAAAAAemJk2amJm4uDjP8ejoaLNGcXGxmQkLs6/XCA8PNzMJCQme4xUVFWaNhQsXmpmqqiozU1paamZ27NhhZg466CDPcZf7rmnTpmYG8BvruS9JJSUlnuPWGiZJycnJZmbFihVmxuW1WF1d7Tnuss65CAaDZqaoqMjMNG7c2MxYcy4vLzdrtG/f3swsW7bMzKB+RETYravKykozk5iY6DluvcdL0qZNm8xMqFx88cVmxnpub9u2zaxhrXOS1KJFCzPjIj8/33O8rKzMrOHyfHBZ61zWVJf1+9Zbb/Ucj4mJMWt88MEHZsZ6/krSlClTzEwgEDAzKSkpnuNTp041a3Tt2tXMuOCKawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4SkR9T+BAEB4e7jleVVW1j2YSGv/+97/NTHV1dUjq/FXFx8ebmaKiojofJxAI1LkGAAB/VqNGjepco0mTJmZmy5YtZiYYDJqZsDD7mo7y8nLP8ZycHLOGi8TERDNj7UElqaCgwMw0bdrUc7y4uNis0bBhQzMD+E1KSoqZKSsr8xyPiYkxayQlJZkZl/Mrl/NKK+Oybrhwma/LmhoXF2dmYmNjPcezsrLMGi7vJfCvyspKM5OcnGxmhg4d6jluvR9K0rPPPmtmxowZY2aio6PNjMvr1XqdtW3b1qzhskeqqKgwMy6PU3p6uue4y/1SWloakozLOhaK557LcdLS0syMy3vJO++8Y2ays7PNzIIFCzzHXV4Hhx56qJlxwRXXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPCViPqewIGgqqqqzjXCwuyfIVRXV5uZBg0amJlVq1Z5jo8dO9assXjxYjNz0kknmZlt27aZmYSEBDNj3Tcuj1FKSoqZOeqoo8zM+vXrzYzL4/S///3Pc/yzzz4zawSDQTMD+E1sbKzneGVlpVmjoqIiVNMBUAeNGzc2M4FAYB/MRAoPDzczcXFxZsbaU0RGRpo1XDJ5eXlmxuW+c9k/Nm3a1HN83bp1Zo0mTZqYGcBvXPb/5eXlnuMu64bLuV5MTIyZcVnHrDWqrKwsJMdxWcdCcZ4s2e8lq1evNmu4PNbYv+Xm5poZ6z3x3nvvNWt8//33Zsalz7Fx40Yz4/IaKigo8Bx36Qm4ZKKiosyMy75k69atZmZfcVl3XR6DLVu21LmG9V4jua27O3bsMDPJyclm5sEHH/Qcd9n39e3b18y44IprAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICvRNT3BPwuEAjUuUZYmP3zgfj4eDOTn59vZoYOHWpmxo4d6zm+fft2s0aDBg3MzOuvv25m7r//fjPz6quvmplhw4Z5joeHh5s1CgoKzMyhhx5qZqKjo83Ml19+aWaaNGniOZ6QkGDWKCwsNDOAC5fXUHV1tZkJBoNmpqSkxGlOXg477DAzU1RUZGaWLVtW57kcqGJjYz3Hy8vLzRouzxns31xe85GRkZ7jiYmJZo21a9eaGZe9S0RE3bfGpaWlZsZlbxgVFWVmXO5fFzExMZ7jLrfJ5f4F/MblHMza27vskTZv3mxmrLVQcjtfsebjsraE6v3Z5b5xyRQXF3uOV1VVmTVcHmvs30aMGGFmbrnlFs/x559/3qyRl5dnZjZs2GBmysrKzIzLvsR6Tbu8nl16GC7zdXk9x8XFeY67nENUVFSYGZd1wWV/47JmWvs6lxou/R2X/qR1jiZJP/zwg5mxelYPPPCAWePuu+82My644hoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvhJR3xPYWwKBgJkJBoMhyViqqqrMTH5+fp2PI0kVFRVmZtCgQZ7jN910k1nDJfPYY4+ZmVtuucXMTJo0ycw888wznuOnnnqqWSM3N9fMLFq0yMxcfPHFZuboo482M6+88kqda3zwwQdmBnDhso656NSpk5k566yzPMdXr15t1nj11VfNzF133WVm7rnnHjMTFmb/DNh6LwnFe82+VlJSUt9TQD2LiLC3kbGxsWamvLzcc7xp06ZmjTlz5piZlJQUM+MiKirKczw8PNysUVZWZmZc6rjsd10ep+LiYs9xl/eAuLg4MwP4TWJiopmxXmd5eXlmjbVr15qZtm3bmhmXc73q6mozY3FZf0K1LmRlZZkZa+/XokULs4bLupuUlGRmQnXejtDr16+fmbGeSwsWLDBrlJaWmpnCwkIzEx8fb2YqKyvrnHE5V3F5fbjsOVzma+37XLiscy63Ozo62sy43DfW7XZZU639peR2zujyvGrdurWZ2bZtm+f4+eefb9Z46aWXzIwLrrgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK9EhLJYIBCoc41gMBiCmYSuTigcd9xxZmbbtm1mZuHChWbm5ZdfdpmSp4yMDDPz4osvmplnn33WzDzwwANm5vjjjzczH330kef4e++9Z9YoKyszMzt27DAzK1euNDMNGzY0M5s3b/YcP+igg8wa8C+X9dJP61jHjh3NzL///W8zk5KSYmZatmzpOf7www+bNR566CEzs2nTJjPjorq6OiR19jf9+vXzHN+yZYtZY8WKFaGaDupBUlKSmUlISDAzJSUlnuPt27c3a0REhGZLGxkZaWas9dtlTYiPjzczpaWlZsZFZWWlmYmLi/Mcj4mJqXMNyW3/s3PnTjMDhIrLfqxFixae4/Pnzzdr5OXlmZno6GgzU1FRYWaioqI8x8PDw80aVVVVZsalTmxsrJnZvn27mdm4caPneO/evc0aq1evNjMu71n5+flmBvVj6dKlZsbaL7icfzVr1szMWHsbye015PKcLCoq8hx32Ze4ZFz2Wi5rqsvtDkUNl3XM5XFyqWPdbpf7xWXfZ63vrnWSk5PNjPXccznXW7RokZlxwRXXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPCViFAWCwQCZiYszLtXXl1dbdZwyURE2DetsrLSzLRo0cLMHHrooZ7j3bp1M2ucffbZZuaCCy4wMytWrDAzL7/8suf48OHDzRqPP/64mXn44YfNzPr1683Mq6++ama+/PJLz/GHHnrIrGE9jpJ02mmnmZm1a9eamZKSEjPTpk0bz/GlS5eaNfDHWWtHMBg0a1RVVZkZlzqhkpCQ4Dn+xBNPmDUyMjLMzIwZM8zMm2++aWYKCws9x4877jizxkEHHWRmGjZsaGZWr15tZj777DMz4yeHHXaYmbnqqqvqXGfx4sVmjYsuusjMwL+aNGkSkjrh4eGe440aNTJrxMXFmZn8/HwzEx0dbWYiIyM9x13Wd5c9qEudsrIyM+Nym6z10Hofkdz2Nk2bNjUzO3fuNDOAC+u8U3I7r4yPj/ccX7hwoVmjWbNmIZlLRUWFmbFERUWZGWudk6TS0lIz47JGuezHvvvuO89xl3N/l+eDy/vaxo0bzQxCz+UxHjFihJlZsGCB5/jEiRPNGnfeeaeZcXmfj4mJMTPl5eVmxnpuW/ssyW3P4fIacmHVcZlLqM6lXXqYLhlr/Q7VfPflY2m9DzRu3NisESpccQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX4kIZbHq6uqQZEJh2LBhZiYuLs7M9O/f38x06dLFc3z16tVmjVWrVpmZiy++2MzcdtttZsby9ddfm5m+ffuaGZf5vvzyy2bm559/NjPvvvuu53hRUZFZIzs728y89tprZubFF180M88++2yd51NVVWXWwB9XWVlZ31Oo0axZMzNz3HHHmZmTTz7Zc3zOnDlmjQULFpiZRo0a1XkuktS8eXPP8Z07d5o12rRpY2YSExPNTPfu3c3MmjVrzExWVpbn+I4dO8waqampZiY9Pd3MdOzY0czk5OSYGev9vHHjxmYN7N8SEhLMTFRUlJkJC/O+jmLp0qVmjejoaDPjskZt3brVzMTHx9f5OCUlJWbGZa3r3bu3mZk3b56ZCcXr2WUf5fKcAUIlOTnZzLicD1rnES7nt61btzYzLuuCy3ytc4SICLsF4DIXlzrl5eVmxmV/Y83HZb4u504u9y/qh8s5WoMGDczMzJkzPcdjYmLMGk2aNDEzLnVcXh/BYNDMWFxeqy7HCVUmEAjUadz1OC5c1oXw8HAzY70PuLxPWPthyW2+Lntvl/lY97HL4xQqXHENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPCViFAWS05ONjMnnXSS53iTJk3MGjt27DAz//znP83MunXrzMwpp5xS5zqtW7c2a8yePdvMnHPOOWbGun8lqaKiwnO8c+fOZo0JEyaYmVGjRpmZyy+/3Mx07drVzPzvf//zHI+KijJrxMbGmpnS0lIzM2jQIDNz9tlnm5mGDRt6jj/55JNmjaeeesrM/JVERNhLXu/evT3HDz74YLNGZmammUlISDAzTZs2NTMNGjQwM1u2bPEcDwaDZo1hw4aZmfDwcDPjcrvXrFnjOe7yHpCbm2tmmjVrZmbi4uLMTIsWLcxMy5YtPcerq6vNGi7P30AgYGZc3vtcWPdfVlZWSI4D/0pMTDQzBQUFZsZ6LllrgiSVlZWZmXbt2pmZTZs2mRnrdrs896Ojo82My77EZd0tLi42M6Hgsv4A+5LL/t/lNVRUVOQ5XllZadZITU01My77G5f5hoLLnsNFeXm5mYmJiTEz7du39xx3WbsjIyPNTKhuN+pHVVWVmZk/f77n+McffxySubi8h7ucZ7i85vPz8z3HXd6fQ5VxOaexzj3Dwuzra13m4nKO68LlMQjFHigU953ktu4edNBBZsbaW7ucb/fv39/MuOCKawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4SoRrsEOHDmbmo48+MjPr16/3nlCEPaXo6GgzU1lZaWYKCwvNzPvvv29mNm3a5Dn+9ddfmzUiIyPNTJcuXcxMaWmpmWnYsKHn+LRp08waF154oZl55ZVXzMyQIUPMTOPGjc3MF1984TnerVs3s0afPn3MzHXXXWdmXBQXF5uZ/Px8z/ELLrjArDFz5kzXKe33+vbta2Yuv/xyMxMIBDzHExMTzRou61hUVJSZsV6rrho0aOA5npGRYdYoKioyMxUVFWampKTEzLRt29ZzvFevXmYN6/XjmsnNzTUzLu8lYWHePyd2ec9yybg8P13WVJdjWe/FTZs2NWtg/5acnGxmXJ5LFpfXqsv+x2X9sd4DJPu57zKX6upqMxMbG2tmXASDwTpnXPbeoVgLgVAKDw8PSZ2ysjLPcZc9ncu5nss+yqWOtY651HBZN0LF5XY3b97cc3zbtm1mjWbNmpkZl8cS/uXyXLLez9555x2zRu/evc1MXFycmXHZC7jsS+Lj4z3Hq6qqzBqhWi9DVccSqvvOZT10YR3LZf/jsn904fJ4JyQkmBnrvc9l3V20aJGZccHuEQAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+EuEajIuLMzOrVq0yMytWrPAcj4+PD8lcGjRoYGYCgYCZGT9+vJlp1qyZ53jHjh3NGl9//bWZWblypZkpLS01M127dvUcb9q0qVnDehwl6b777jMzQ4cONTOpqalmpk2bNp7jS5cuNWts2bLFzFxxxRVmJjc318wUFhaamZycHM9xl+d4WVmZmTlQfPfdd2Zm+fLlZiY5Odlz3OX10bhxYzOTlJRU57lIUuvWrc2Mta4mJCSYNVwyoVJZWek5vnDhQrOGy1rokomMjDQzO3fuNDPh4eGe4xUVFWYNl/c+l+eMy3uJyxplPSdcbhP2b9HR0WYmGAzWOeNynIgIe0u7ceNGMxMbG2tmrPdWl72si5KSEjPjsv647Hc3b97sOe6yFroIVR3AhcvexWXtsPYLLud61j5AksLC7GvKysvLzYx1m1zWBGsv5nIcye12u9ymJk2aeI7n5+eH5Dgu51fwr+zsbDPTtm1bz/HrrrvOrPG///3PzDRv3tzMrF+/3sy47P+tfYnLa76qqsrMuAjFulBdXW3WcHk9u6ypLvsSl56KtZd1OS9yeZxc7huX252YmGhmNmzY4Dnu0vMI1fkgV1wDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFciXIPZ2dkhOWCvXr08x3fs2GHWqKysNDMREfZNe/31183MGWecYWZatmzpOT558mSzxrx588zMrFmzzEzHjh3NzKmnnuo5PmLECLPGunXrzEzTpk3NjHXfSVJGRoaZGTNmjOf4iSeeaNYoKyszM40aNTIz5eXlZiYuLs7MREZGeo63bt3arOFymw4U0dHRZmbbtm11zqxcudJ5TnUVHh5uZmJiYsxMUlKS53hUVFRIjmM9Z13rWJnq6mqzRiAQCEkmVKz3rbAw++fILvMN1XPG5VhVVVWe48nJyWYNl9ct/MvleZKXl2dmGjdu7Dnusudw2U9s3rzZzLisY/n5+Z7jLvsAl+O42Lhxo5lJSEgwM9u3b/ccb9asmVnDWhMkKTY21swAoRKq93nrNe1yburyfueyF3B5nVm32+U4LufSLpmKioqQZKy1o6CgwKwRqvsX/uXyPt+zZ0/P8RdffNGs8dprr5mZs846y8ysXbvWzLi8zkpLS82MxeUcIhgMmhmX15l1LueyJrjMxeWc0WX9djnWvuKyRrk8H4qKiupc56uvvjJruPS9XHDFNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8JcI1uH37djPz+OOPm5lhw4Z5jjdv3tys0ahRIzNz+OGHm5k33njDzLz11lt1zqxevdqs8b///c/MuDjppJPMzJNPPuk5fsstt5g1duzYYWbi4+PNzFVXXWVmHnzwQTNz4403eo7PmzfPrHHUUUeZmalTp5qZgQMHmplvvvnGzKxYscJz/KOPPjJrZGVlmZkDhcvzLSYmZh/MRKqqqjIzBQUFZqaystLMlJWVmZlNmzaZGSDUEhISzExcXNw+mAn2lrAw+/oHl3WspKTEc3zJkiVmDZf3gKioKDPjcpvCw8M9xwOBQEjmYt0vktueeMOGDWYmGAx6jpeWlpo1XG53gwYNzAwQKi7rgsvz1uKyzlVXV9f5OJIUEWGfvlvHio6ONmu47FNjY2PNjAtrTXU5VmRkpFnD5b4L1W1C/XB532zdunWdj7NgwQIzc/nll5sZ671XknJzc82MtefOz883a7jsf1yEYq0L1Vz2pVC8l7hwec6E6j3Jeg+94oorzBrl5eVmxsX+94wAAAAAAAAAABzQaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHwlIpTFPvjggzpnmjZtatZo2bKlmTnqqKPMzJo1a8zMCSecUOf5dOvWzaxx1llnmZmcnBwzExFhP6Rt2rTxHL/88svNGvn5+SHJxMbGmplOnTqZmTFjxniOh4XZP6Pp27evmbnvvvvMzD/+8Q8zg9DbvHmzmQkPDzcz8fHxda7h8nxr3LixmQmV6upqz/FAIGDWcLndoRIMBvfZsSwua6rLfEPxGLhwee65ZFzmU1lZ6TneqlUrs0ZWVpaZgX9t2bLFzFRVVZmZ0tJSz3GX9bKkpMTMuKxjkZGRZsZ6n0hKSjJruKwtDRs2NDPWfSdJ5eXlZsa6j13uX2udk9zmC4RKRUVFSOoUFRV5jltrgutcXN57XTLW69Vl/QkVa68gua0dcXFxnuMu90teXl5I5gL/Wrp0qZk58cQT63ycOXPmmJmEhAQz4/K8ddlHbdu2zXPcZW/jcj4TqvNB63aH6nzb5fXsknF5nFzmY3F5DFz2dC7vNy73sXUsl7mECldcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8JaK+J/BbOTk5IcnMnz/fzAQCATPz5Zdfmpl77723zsdJSkoyMy+99JKZiYiwH9I2bdp4jn/wwQdmjcrKSjMTKvHx8WZmxowZdT5OWJj9c5yoqKg6Hwd7RzAYNDMuz9u8vLw6z8XluRQeHm5mXNYOlzpxcXFmJhRzqa6urvNxJPv+c3msXebrwuU2VVVV1bmOy3PGhct9EyrW+83XX39t1tiX80Xouaw/GRkZZqasrMxzPDEx0ayRkpJS5+O4Zqx9aGxsrFkjNzfXzMTExJgZFwkJCWampKTEc3zDhg1mjVA9BkCouOz7XN6HXF5DFpf9hMtr3nqtSlJ0dHSd5xKqTKj2N9a66nKOlpycbGYiIyNdpwQfevDBB83Mrbfe6jnusn/t27evmVm9erWZcXnNFxcXmxlrTx6q53Wozget/aPL/tLl/Mtlvi5rVCjeS1zm4pJxOa93ec9yuY8/+ugjM2MJ1XsAV1wDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFcCwWAw6BQMBPb2XADUA8clwPdYo4ADE2vU/i08PNzMVFVVeY4PGjTIrHH66aebmeLiYjPTuHFjM5OYmOg5vmbNGrNGdHS0mcnNzQ1JncjISDMzZ84cz/FXX33VrPFXxRrlX7fccouZueGGG8zMihUrPMfffvtts4bLa7WystLMFBQUmJny8nLP8bZt25o1Nm/ebGastVCSSkpKzExCQoKZadCgged4jx49zBqHHnqomZk9e7aZOffcc82Mn7BG1TZixAjP8UsuucSssWPHDjPjMl+X93nr9Sy5rS8Wl+eJy21yqRMWVvfrZ0P1vN5XtztU83V5n3DJpKammpkhQ4Z4jldUVJg1XDg9Z0JyJAAAAAAAAAAAQoTGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAVyLqewIAAAA4MFVVVdW5RmRkZEgyLioqKsxMSUmJ53hUVJRZIyzMvnbE5b4rKioyMwkJCWYmLy/PzAD7m/vvvz8kGcuoUaPMTHV1tZkJBoNmxmXtsNagUK0/LhmX2+2y7rZo0cJz/IQTTjBrAJL07LPP1mlckpo0aWJmnnjiCTPTsGFDM+PyHp6cnOw57rJHCgQCZiY8PDwkdaxMqNaN8vJyM1NZWWlmXG6TxeU2uVi9erWZiY6ONjP5+flmxuU+3le44hoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvhJR3xMAAADA/icQCJiZiAh7q1lRUeE5/uOPP5o1zjnnHDMTHh5uZsLC7Gs6qqqq6nyc6upqM9O8eXMz46KsrMzMuNzHFpfng4tgMBiSOsC+kpqaamY2b95sZmJjY82My9pRVFRkZizWuuzKZT10UV5e7jmekZFh1liyZElI5oL9m/WctN7jJWnLli1mZtiwYWbGZe1weQ1Z7/Oheh26cNkLWOuYyz7AZS0M1X7CZU215hOqPZLLvjolJcXM7Nixo85zcblNoXoMuOIaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL4SUd8TAAAAwP4nGAyamcrKyjofZ9WqVWbmoosuMjODBg0yM/369TMzvXr18hwvKCgwa8TFxZmZNWvWmJlZs2aZmU8++cTMbNy40cxYXJ4PwL4UCATMjMvzNizM+1qviooK5znV5TiSVFVVVec60dHRZg2XTFRUlJlxma+L6upqz/HY2NiQHCdUzxn4V6iek6Gwbdu2+p4C9pFQrRvl5eVmZvPmzSE5lmVfroVccQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX4mo7wkAAADgwBQMBvfJccrLy83M1KlTQ5KxnHXWWWZm/vz5Zmbt2rV1nguAuouMjPQcb9u2rVmjtLTUzFRUVLhOyVMgEPAcr66uNmu4ZFzWd5c6VVVVZmbnzp2e482aNTNrAAD2T1xxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwlYj6ngAAAABQF2Fh9rUYwWAwJBnL9u3bzUxubm6djyNJgUDAzITiNgH7o1A998vKyjzHx44da9bo3r27mTnmmGPMTFZWlpmxbnezZs3MGnl5eWamefPmZiY7O9vMhIeHm5nU1FTP8VmzZpk1XLCmAoD/cMU1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHwlEAwGg07BQGBvzwVAPXBcAnyPNQo4MLFGYX9z0kknmZmffvrJzKxfv97MhIXZ16BUV1ebGfx5rFEIlczMTDPTo0cPMxMbG+s5vnLlSrPGokWLzMwhhxxiZjp37mxmIiIizMx///tfz/GdO3eaNf6qWKMA+JnLGsUV1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwlUAwGAzW9yQAAAAAAAAAANiFK64BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSufeLJJ59Uenq6/vGPf/zpGk888YTS09NVVla2x/ENGzYoPT1dkydP/tPHcDkOgL+W0aNHKz093fO/Cy+8sL6nCeAvaE/rU2Zmpk466SQ99dRTKi8vd6712/3PhRdeqLPPPntvTR0APC1cuFD//Oc/deyxxyozM1Pdu3fXkCFD9Prrr9dkBgwYoNGjR3vWGT16tPr27bu3pwvAh/bmedy+WH/uv/9+XXzxxX/6+7F/iKjvCUAKBoN6++23lZ6ers8++0y5ublKTk6u72n5Rnl5uXr06KGPPvpILVu2rO/pAPiNf/3rX7rxxhtr/n3XXXdpyZIlevPNN2u+FhkZWR9TAwClpKRo2rRpNf/Oz8/XnDlzNH78eK1atUoPP/xwPc4OAP64uXPn6pJLLtHJJ5+sxx57TI0bN9b27ds1depU3XnnnSouLtYll1ziVOtf//qXKioqzNwFF1ygM888U2eeeWZdpw/AJ+r7PK6u68/s2bM1ePBgSdLjjz+ujRs3aty4cXtlrqg/NK594JtvvlF2drbeeustnXfeeXrvvfe4OvFXfvzxR6fFDED9SExMVGJiYs2/o6OjFR4ersaNG9fjrADgF2FhYbXWo8aNG6t9+/basWOHnnrqKd1888066KCD6nGGAPDHTJ48WU2bNtX48eMVCAQkSc2aNVNmZqZKS0u1ZMkS51q/3sP9nsrKSi1evJimNXCAqe/zuLqsPzk5OVqxYoX69esnSfr+++/VtGnTvTJP1C8+KsQHpkyZou7duyszM1PHH3+83nrrrd0yF154oUaOHKmPPvpIp5xyirp06aK//e1vmjVrlmftcePGqUePHlq8ePEex9esWaNrr71W/fv3V5cuXXTmmWfq888/d5r3ihUrdM4556hLly7q16+fJkyYUGs8JydHN954o3r37q3MzEwdd9xxevzxx1VZWVmTCQaDeu6553TiiScqMzNTvXr10rXXXqt169ZJkt5++22dd955kqSBAwfS0Af2U7t+q2TWrFkaOHCgzjrrrFpjp556qjp37qzDDjtMl112Wa01a9f3rlq1qlbNAQMG6J///GfNv19//XWdeuqp6tatmw4//HBdeumltU7cgsGgXnrpJQ0ePFjdunXTkUceqTvvvFP5+fk1mdGjR2vw4MGaPHmyevXqpfvvv39v3B0AfKBjx46SpI0bN+7xYz/mzp2r9PR0ffnll071ysvL9dBDD2nAgAHKzMzUkUceqdGjR2v79u2SpEcffVSdO3dWYWFhre9buHCh0tPT9emnn0qStm7dqptvvlkDBgxQ586dNWjQoFpXPklSenq6nn32WV155ZXq3Lmzli9f/qfuAwD7p9LSUlVVVe3x4p4xY8Zo/Pjxtb727rvv6vjjj1dmZqYGDRqkBQsW1Iz99lf1BwwYoHvvvVe33nqrunbtqkmTJikjI0MlJSW69dZblZ6evvduGID9hnXutcveWn+++uorNW7cWOnp6RowYIC++eYbTZ06Venp6Zo7d64kadWqVbrqqqvUs2dPZWZm6pRTTtGkSZNqzS89PV1PP/20HnnkEfXt21ddunTRRRddpLVr14b4HsOfReO6nu3cuVMzZsyoaeIMGTJES5cu1U8//bRb9ueff9bbb7+t8ePHa8qUKYqNjdXNN9+skpKSPdZ+6aWX9Nprr+mpp55SZmbmHo99wQUXKCsrSw8//LCmTp2qnj176uqrr9b//vc/c+733nuv/v73v+vdd9/V6aefrkceeUTTp0+XJJWVlemiiy7S0qVL9fDDD2v69Om6/PLLNXHiRD344IM1NR5//HE9+uijOu+88/T+++/rP//5j9atW6fhw4erqKhIp5xyikaNGiXplwb/E088Yd+pAHzrmWee0X333Vfzg64333xTt956q4477ji98847eumll1RRUaGLLrpImzdvdq47Z84c3X333brkkkv0wQcfaNKkSWrQoIEuvfTSmjXy6aef1rhx4zRo0CBNmzZN48aN0+zZs3XNNdfUqrVrXZ40aZKuvPLK0N14AL6y64SkWbNmIal3++2367///a+uu+46TZ8+XWPHjtXcuXN1xRVXKBgM6tRTT1V5ebm++OKLWt83ffp0JScn6+ijj1Z5ebmGDx+u+fPn6+6779Z7772nwYMH6/bbb9c777xT6/umTJmiww47TB9++KHatWsXktsAYP/Qv39/5eTk6Pzzz9fHH3+sgoKC383+8MMPmj17tp566im9+uqrqqys1KhRo1RdXf273zNr1izFx8frvffe01lnnaXXXntNknTbbbdp9uzZIb89APYvLude0t5df2bPnq2+ffsqEAjozTffVEpKik4++WTNnj1b3bt31/bt23X++ecrNzdXzz77rN5//30NHjxYY8aM0SuvvFLrmK+//rrKy8s1adIkTZw4URs2bNDVV1/tOU/sOzSu69m7776ryMhInXLKKZKk3r17q2XLlnu86nrz5s0aN26cOnXqpPT09JoX4a6rk3/to48+0vjx4/XQQw+pT58+ezz2lClTtH37dj3++OPq2bOn2rdvr9tuu63mKh7L8OHDdfTRR6tdu3YaNWqU2rRpo/fee0+S9Omnn2rt2rUaN26cjjzySLVu3VrnnHOOzj777JpFoby8XC+//LKGDBmi4cOHq23bturZs6fuu+8+bdq0STNmzFBMTIwSEhIk/fIZlXz2N7B/O+WUU3TEEUfU/PrZxIkT1b9/f11//fVq3769OnfurIcfflilpaV6++23nesuXrxYsbGxOu2009SiRQt17NhRY8aM0bPPPqvw8HBVVFTo+eef1+DBgzVixAi1bt1a/fv312233aa5c+fW+sl/Tk6ObrnlFqWnp7PmAAegiooKzZ49Wy+88IJOOOGEkDSuc3JyNG3aNF111VU6/fTT1bp1ax199NEaPXq0lixZovnz56t9+/bKyMjQRx99VPN9wWBQH330kU466SRFRUVpxowZWrVqlcaMGaP+/furbdu2GjFihAYMGKCnn3661jETExM1YsQItWzZUlFRUXW+DQD2H+eee66uvfZarVixQtddd5169eqlM888Uw8//LDWrFlTK1tUVKQxY8YoLS1N3bp105AhQ5Sdna2tW7f+bv2ioiLddtttat26teLi4tSwYUNJv6w7fBQcAOvca5e9tf5UV1drzpw5NVdrp6SkKCwsTDExMWrcuLGioqL05ptvKi8vT48//rh69Oihtm3b6sorr9Qxxxyz21XXcXFxuvnmm3XwwQfriCOO0MiRI7Vy5UotXbo01Hcd/gQa1/Xsrbfe0sknn6z4+HhJUiAQ0Jlnnqn3339/t79036ZNG6WkpNT8e9cL+Ne/5i5J3333nW666SbdfvvtOuGEE3732IsWLVLr1q3VunXrWl/v3bu30+eiHXbYYbX+nZ6ertWrV0v65XOpo6Oj1blz51qZ7t27q6SkRKtXr9bq1atVVFSknj171sp06tRJ0dHRe7zqHMD+7de//VFYWKi1a9futgakpqaqVatWf2gN6Nu3r6qrqzVs2DBNnjxZa9asUVxcnLp27aqoqCitWrVKhYWFu/3V6t69e0tSrWNFR0crLS3tz9w8AD60fft2de/evea/rl27auTIkTr++OND9gd8Fi9erGAwuNt61r17d0n/b43529/+pq+++kpFRUWSpPnz5ysnJ6fmDwv98MMPioyMVK9evWrV6dOnj9auXVvzfZL2+Nt0AP4aAoGArrnmGs2ePVsPP/ywzj77bJWUlOiZZ57RKaecUnOFovTLudWvf7i163zy1+vJbx166KEKC6NVAGDPrHOvXfbW+rNo0SLl5eXtdm73az/++KNat26tJk2a1Pp69+7dtX79+lof3XbYYYfV/L0AScrIyJAkZWdne84D+wZ/nLEeLVy4UCtWrNCKFSv2eIX1jBkzaq7Eln75KdCv7XphBYPBWl+/7rrrVFlZ6flTLOmXplFWVlbNSdUuFRUVqqioUHl5uecVPElJSbX+HRsbW/NrIYWFhYqPj6/14pdUc/V0UVFRzbx/+4H8YWFhiouL81zMAOyffv1637VZ2LUu/FpCQsIfWgM6deqk119/XS+88IIef/xx3X333erQoYNuuOEGDRw4sOZYt99+u+66667dvv/X66XLHwkBsP9ITk7W66+/XvPviIiImqtxQmXXGvPb9ePX+x5JGjRokB588EHNnDlTgwYN0vTp09W6dWv16NGjpk5FRcVuFwfs+vsgW7durbnY4bf7MAB/PYmJiRo0aJAGDRokSVqyZIluuukmjR07VieddJKkX87Rfu33ziF/jfUFwC6/7Rd9//335rnXLntr/Zk9e7Y6deqkRo0a/W6msLBwj+d1v96b7fr/3+Z29d5+e5Eo6geN63r05ptvqm3btnr00Ud3G7vvvvv01ltv1Wpcuxo1apRKSko0fvx49e7dW4cffvgec0lJSWrVqpUmTpy4x/GICO+nR1FRUa2FqLi4uNbJ1K7m9K+b17s+fy0pKalmsfrtZ7JVV1erqKiI5hFwgNu1UfjtHyrb9bUWLVpI0m4/ANvlt43t9PR03X///QoGg/rxxx81ceJEXXvttZo+fboaNGggSbrpppvUv3//3Wqx3gAHrvDwcLVp08bM/fYkqri42PkYu06yfrun+fW+R5KaNm2qXr161Xw8yMcff6xzzjmnVp2YmJjdPs96l1B9HjeA/VtZWZmkX35L7NcyMjJ0ww036Oqrr675TVgAqIvf25N4nXu1bdt2r85p1+dbe0lKStKmTZt2+/quvdmvL5767Xnlrn/vOodE/eL3f+pJcXGxpk+frr/97W869NBDd/tv8ODB+uabb/b4QrOceeaZuuSSS9SnTx/deOON2rlz5x5z3bp106ZNm5SQkKA2bdrU/BceHq5GjRqZv54xb968mv8PBoP66aefdMghh0iSunTporKyMi1atKjW98yfP18JCQlq27at2rVrp8TERH377be1MosXL1Z5efluHzPi9VM5APufhIQEdejQYbc1YMuWLcrKyqpZA3Y1lXfs2FGTWbdunXJzc2v+PX/+fP3www+Sfml0d+nSRffee6+qqqq0YsUKtWvXTklJScrKyqq13rVs2VKVlZW1PoYJwF9PUlJSrTVG+uU341xlZmYqLCxst/Vs/vz5klRrT7Pr40K+/vprbdu2TaeddlrNWLdu3VRaWqqSkpJaa1VMTIySkpL4LGsA2rJli3r27Lnb597vsmHDBkm//KAs1DgfA/56fr0f2XUhgHXutTfsWn/y8vK0aNEi9evX73cz0i89qaysLOXk5NTK7Pq7I7suupS02/5t10fnHnzwwSGbP/48Gtf15IMPPlBRUdHvXlF9/PHHKzw8/A/9cbJfCwQCGjdunCorK3XbbbftMXPmmWeqQYMGuu666zR//nxt2LBB06dP19ChQ/XEE0+Yx3jllVc0e/ZsrVmzRvfff7+ys7N1xhlnSJIGDhxY88ce582bp/Xr12vSpEl68803dckllygyMlKRkZG65JJL9NZbb+m1115TVlaW5syZo9GjR+vggw/WcccdJ+n//ZRr1qxZWr58+Z+6PwD40xVXXKGvvvpKTz75pNauXauFCxfq+uuvV3Jyss466yxJv3wMSEREhJ5//nmtWbNGCxcu1O23317rhOyLL77QyJEj9cknnyg7O1urV6/WhAkTFBMTo86dOysiIkKXX365Jk+erFdeeUVr167V0qVLdeutt2ro0KG7bWgA/LV06dJFGzZs0BtvvKGsrCy9/fbbmjVrlvP3N27cWGeccUbNX63PysrSZ599prFjx+qII45Qly5darInnniiqqqq9Mgjj6h79+61rgY/9thjlZaWplGjRumbb75Rdna2Zs2apQsuuEB33HFHSG8zgP1TkyZNdP7552vChAkaO3asFi5cqOzsbC1btkwTJ07UI488osGDB+/2d4zqYtf52Lx587Rs2TKVlpaGrDaA/Y917hVKv11/vvjiC0VHR9d8zNouSUlJ+umnn7R06VJt27ZNZ555ppKTk/XPf/5TixYt0po1a/T444/ryy+/1IgRI2p9b35+vsaOHatVq1Zp7ty5+s9//qMuXbqoffv2Ib0t+HP4qJB68tZbb6ljx46/+0Jo0KCB+vbtq6lTp2rkyJF/6hiNGzfW2LFjdeWVV+qVV17RgAEDao0nJyfrv//9r8aPH6+rrrpKxcXFatasmYYPH64rrrjCs3Z4eLjuvPNO3X333Vq6dKmSk5N166236thjj5UkRUVF6cUXX9T999+va6+9VkVFRWrRooVGjRql4cOH19QZOXKkoqOj9fLLL+u+++5TYmKijjrqKN100001VxUdffTR6tGjh8aNG6e0tLQ/3cwH4D+nn366qqur9eKLL9Zsdnr16qUxY8bUXAXdvHlz3XPPPXrqqad02mmnqW3btrrlllv0+OOP19S5/vrrFR4ervvvv19btmxRXFycDj30UE2cOLHmV+uvvPJKxcfH67XXXtMDDzygqKgoHX744Xrttdf2ylVJAPYfF154oX7++WeNHz9elZWV6tevn26//Xadf/75zjXuvvtupaSkaPz48dq6dasaNmyo448/XjfeeGOtXFJSko455hh98sknuvPOO2uNRUVF6aWXXtL48eN14403Ki8vT6mpqRo0aJCuu+66kNxWAPu/0aNHKyMjQ2+++aY++OAD7dy5UzExMTrkkEN0yy23aNiwYSE9Xmpqqs477zy99dZbmjlzpt555x0+ugj4C3M59wqV364/AwcO1BFHHKHIyMhauSuvvFJjxozRueeeq7Fjx+rkk0/WpEmT9MADD+iSSy5RWVmZDj74YN1///06/fTTa33vaaedpoiICF100UXKz89X9+7dNWbMmJDeDvx5gSC/7wMAAAAAAADgLyQ9PV1XXHGFRo0aVd9Twe/go0IAAAAAAAAAAL5C4xoAAAAAAAAA4Ct8VAgAAAAAAAAAwFe44hoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArEa7BQCCwN+eBEImIsB/SyspKz/GDDjrIrHHbbbeZmeLiYjOTnJxsZqKjo82Mdbvj4+PNGgUFBWZm586dZmbJkiVm5ocffqhzpqyszKzhIhgMhqROfWON2ru6detmZv7v//7PzMydO9fMJCUleY7v2LHDrLF582Yz06JFCzNTXl5uZlzWsX/9619mBnv2V1qjwsPD63ycqqqqOtcIpZ49e5qZQw45xHPc5TYNGDDAzLi8h3/44YdmZuPGjWbGEhcXZ2YOP/xwM9OlSxczs2zZMqc5efn222/NzHfffVfn47hyea3sq9fCX2mNwt7Vtm1bM9O0aVMzs2LFCs9xl3Mrl/VnwYIFZiY7O9vMYO9ijcLgwYPNTMOGDc3M999/b2aKiorMjLUHatSokVkjLy/PzGzfvt3MdOrUycy4rLsvvfSSmdlXXF4rfloXXObCFdcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF8JBIPBoFMwENjbc0EIREVFmZny8nLP8TPOOMOscfvtt5uZ4uJiMxMbG2tmXERHR3uON2jQwKwRExNjZho3bmxmtm7damYqKirMTFFRkef42WefbdZYuHChmXFcAnyPNWrveu2118zMeeedtw9m4qawsNDMJCQkmBmX17PLutCoUSPP8R07dpg1/qpYo+pHt27dzMyJJ55oZrZt22ZmrP1CaWmpWaO6utrMnHzyyWamd+/eZsa6TQ0bNjRruOzXCgoKzMySJUvMzH//+18zY61jhx9+uFnD5bFev369mXn//ffNjMt9s6+wRsFFs2bNzEzTpk3NzJVXXmlmOnbs6Dnu8pzNz883M6NHjzYzmzdvNjO5ublmBn8ea5R/ZWZmmpmhQ4eamZNOOslzfPny5WaNdu3amZns7Gwzk5eXZ2YuvPBCz3GXHtGqVavMzDvvvGNmMjIyzIzVl5GkQw891HP8888/N2tMmTLFzHz55ZdmZn/jskZxxTUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfCUQDAaDTsFAYG/PBSEQHh5uZqqqqjzHx4wZY9bo37+/mSksLDQzcXFxZqa4uNjMJCcne45HRESYNSIjI+t8HEnKz883M5WVlWYmMTHRc/yoo44ya2zevNnMOC4BvscatXdt3LjRzLi8zjZt2mRmwsK8f6YaFRVl1nBZf+Lj481Mbm6umTniiCPMzCmnnOI5/uGHH5o1/qpYo0Lv2GOPNTPHHHOMmfnss8/MTKNGjcxMdXW157jLPqCsrMzMuKxjLvuohg0beo7HxsaaNQoKCszMtm3bzIyLzMxMM2PtXay9o+S2p+vdu7eZWbx4sZmZOnWqmdlXWKP2b0lJSWYmLS3Nc9zlud+gQQMz891335mZHj16mJn333/fc9xacyVp+PDhZmbmzJlmpm/fvmZm3bp1ZsZaM5cvX27WcDn/OhCxRtWPf/3rX2bmsMMOMzNFRUWhmI4pOjrazOTl5ZmZb7/91szk5OR4jpeXl5s12rRpY2YqKirMjMt+12XPZmWaN29u1nA5l3bZp44ePdrMrF692sxYr7lQrS0udbjiGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKxH1PQG4CwQCZqaqqqrOxzn00EPNTFlZmZkpLS01M+Hh4U5zskRFRXmOR0TYT/X4+HgzExkZaWZcHqeYmBgzs23btjqNA6HUrFkzM7N+/Xozk5KSYmYqKys9x6urq80aLmuLy2ve5fWcl5dnZnr16uU5/uGHH5o1gFDp2LGjmVm4cKGZiYuLMzMu75v5+fme4y6vZ5e5dOjQwcwUFxebmVDsXaKjo83MIYccEpK5uGQaNWrkOV5SUmLWcLlNs2bNMjN9+vQxM4CLtm3bmhmXdcFao1z25KtXrzYzPXr0MDMue6CrrrrKczwszL52bdGiRWbGZU83c+ZMM3PQQQeZmYSEBM/xAQMGmDXmzJljZgoKCswM9m/W89/lNeZyXnT88cebmZ9++snMNGjQwMxY77+5ublmDZd+ScOGDc1Mq1atzIy153DZ0+Xk5JgZl/2utbZIbmvd0qVLPcddemMuc4mNjTUz1113nZn5xz/+YWaCwaCZ2Ve44hoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvhJR3xOAu0AgYGaCwWCdj9O0aVMzs2PHDjOTkJBgZiIi7KdgeHi4mYmJifEcr66uDslxqqqqzIzLsaKjo81Mw4YNPccrKyvNGoCLLl26hKTOzp07zUxUVJSZSUxM9ByPjIw0a7i8DisqKsxMQUGBmXG5TQcddJCZAULB5fmYnJxsZlzep5YtW2Zm8vPzzYylrKyszjUkt/dwl4y1X3Cp4cLlfd4l4zKf1NRUpzl5cVmbrb2NJMXFxZmZVq1amZmsrCwzg/2Xy/OtXbt2ZsbleWKth6WlpWYNl/OMlStXmpkGDRqYmeXLl3uOu5yjJSUlmZkNGzaYmfj4eDPjso5t3brVc9zlMUhPTzcz3333nZnB/i0U/ZLOnTubmfLycjPjsi64nNPk5uZ6jjdq1MissX37djPjsn889NBDzczixYs9xzdv3mzW6NChg5lx6WsVFxebmf/9739m5rDDDvMcd+l7uezXXPbEGRkZZiYzM9PMWI/TvsQV1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwlYj6ngDchYXZP2eorq42M5mZmZ7jubm5Zo3o6Ggzk5CQYGZKS0vNTFxcnJlJTEys83EiIuyXQ2VlpZlx4fJYrl69OiTHAiwZGRkhqRMfH29mgsFgnTMVFRVmjUAgYGZc1iiXOi63Ozw83MwAodCjRw8zs3btWjPTu3dvM7NmzRozk52dbWZatWrlOV5WVmbWcBGq12FVVVWdxl3n4rIvCVUda5/ksr90eZxc9j/FxcVmxuV5npWVZWaw/+rVq5eZKSgoMDMxMTFmxtqXuLzGXF5DLucZ27ZtMzPWPiklJcWs8fPPP5uZ2NhYM+Oy73M5T4uKivIcd3msU1NTzUz79u3NzKpVq8wM/Mva27s8Z0eMGGFmrOesJHXs2NHM5OTkmBnrNeTyfuiydyksLDQz7dq1MzNHHnmk5/iSJUvMGm3btjUzLmvqli1bzEzr1q3NTElJief4vjw3dXkODxgwwMwsXrzYzOwrXHENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+JqO8JwF14eLiZqaysNDMdO3b0HI+KinKek5eKigozExFhPwVdMtHR0XWei4uqqqqQ1HF5nFatWhWSYwGW9u3bh6ROUVGRmTnooIPMTFxcXJ3GJam8vNzMlJWVmZnS0lIzExsba2YKCgrMDBAK3bt3NzObN282M59++qmZueyyy8zM6NGjzUx1dbWZscTHx5sZl/feULzPuxzHhbW3kaTIyEgzU1hYWOe5uKzvp59+uplxuX8///xzM3P44YebmXfffdfMwJ+Sk5PNjMv5yvbt281MMBg0M9b7vMsa5rJ3cdlzxMTEmBlrXXBZo1zWllCdm7o8BiUlJZ7jqampZg0XLns67N9CsedweV679B9c1jGX903r9eFymxMSEsxMXl6emcnOzjYz1ms+JSXFrOFy3y1evNjMHHLIIWZmy5YtZmbHjh2e4y73b6ieDy7r2CWXXGJmHn/8cTOzr3DFNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8JaK+JwB3FRUVIalzxBFHeI4Hg8GQzKW6utrMREVFmZnw8HAzU15eXqdxSQoEAnU+juR2uyMjI81McnKymbG43HdAqJ4nYWH2z0KLiorMTGxsrOf4Z599Ztbo37+/mamsrDQzLq/niAj7rXTnzp1mBgiF1q1bm5mNGzeamW+//dbMjBw50syccMIJZubzzz/3HG/UqJFZw+X1XFVVtU/quBzHZd11mYuL6OhoM7N+/XrP8cMPP9yscfrpp5uZYcOGmZnExEQzE4o9EvyrV69eZsZlHYuLizMzLnty67XoslcoLi42My6vVZfzFWsN2rZtm1nD5RytrKwsJBmX17w1H5c11WUuLo9TRkaGmVmyZImZgT9ZvRJJSklJMTOLFy82M4MHDzYzLs/b7Oxsz/GWLVuaNdatW2dmrHM0yW0PtGXLFs/xzp07mzWysrLMjMvjtHr1ajNTUlJiZlq0aFHnGs2bNzcza9euNTMu++bNmzebGT/himsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+EpEfU8Av4iIsB+KysrKkByrc+fOnuNVVVVmjdjYWDMTGRlpZsLC7J+dJCQkmJmysjLP8aioKLNGUlKSmSkqKjIzDRo0MDNr1641M3feeaeZAUIhOjo6JHVc1rHw8HAzY70WH3nkEbPGsccea2YaNmxoZrZt22ZmXNY6l7UDCAWXvYLLa75Zs2ZmZtasWWbm7LPPNjMffvihmbHk5uaamVDtoyzl5eVmxmWvtX37djNj7X8kqXnz5mZm/fr1nuMvvviiWeOHH34wMz///LOZ6dGjh5lxebyt9xuXxwD1Y968eWamdevWZqZdu3ZmxmVdsPYCLvsfl3Mel7m4PG+DwaDnuMt7gFVDcrtNLhmXdSw+Pt5zPCYmxqzRqFEjM5OVlWVmXNZm7L/atGljZlJSUsyMSw/DRZMmTeqcycvLM2u49EsCgYCZcVk7DjroIM/xH3/80axRUlJiZhITE81MixYtzIzLmlldXe057vI4uuzXXLjsQ10ep9TUVM9xl/PkUOGKawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr0TU9wTwi0AgEJI67du3NzOpqame47m5uWaN2NhYMxMVFWVmIiLsp6BLnaqqqjrX2Llzp5lJTk42MytXrjQzAwYMMDOhYN0vgCQ1atQoJHWKi4vNTGJiYp2P89FHH5kZl+d+ZGSkmQkGg2bGZX0JDw83M0AotGjRwsx89913ZiYmJsbMvP/++2YmPT3dzPTu3dtzfPHixWYNF9XV1SGpY73mXdaE8vJyM+OyjiUkJJiZ7du3m5nu3bt7jrus77NmzTIzPXr0MDMu66XLfqxVq1ae42vXrjVroH64nIu4ZDZt2mRmLr/8cjNjvc4mTJhg1mjWrJmZcblNjRs3NjMFBQWe49ZrQ5J++uknM+Oy/rjso/Ly8syM9T4xePBgs8bDDz9sZpYvX25mKioqzAz2XxkZGWbG5TnbvHlzM5OdnW1mXPYU1rnc0qVLzRopKSlmxuW577JfsPY3LueLLVu2NDNlZWVmxmUd+/zzz81Mp06dPMdd9i0bNmwwM02aNDEzLs8rlz5cZmam5/jMmTPNGqHCFdcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8JWI+p4AfhEWFpqfIQwcONDM5OXleY5XVlaaNfLz881MVFSUmYmPjzczFRUVZsa6/8rLy80aLvNt3ry5mWnVqpWZcWHdN6WlpWaNqqqqkMwFB7a4uLiQ1AkEAmYmMTHRzGzYsKHOc9m0aZOZadeunZmJjIwMSWbbtm1mBnARHh7uOR4RYW/tiouLzYzLa9WlziWXXGJmvv32W89xl72Cdb9IUm5urplx2Y9FR0ebGYvL4xQbG2tmXO4bl/WnTZs2nuNt27Y1a+Tk5JiZhIQEM5OVlWVmXO6/1NRUz/G1a9eaNbB/27p1q5l59NFHzcxhhx3mOZ6UlGTWKCsrMzPV1dVmxuW8yNr/h2otdNn/uLxPuNSxznvuu+8+s8bixYvNDDBgwAAzU1JSYmZcnvsur7NmzZqZmYYNG3qOu/RCXN4TXfYlLu/z1u12WQtd1o05c+aYmVNOOcXMBINBM2O933Tq1MmskZGRYWaWL19uZlz2qS7vW0cccYTn+MyZM80aocIV1wAAAAAAAAAAX6FxDQAAAAAAAADwFRrXAAAAAAAAAABfoXENAAAAAAAAAPAVGtcAAAAAAAAAAF+hcQ0AAAAAAAAA8BUa1wAAAAAAAAAAX6FxDQAAAAAAAADwlYj6nsBfQViY/fOBsrKykBzr+OOPNzPh4eGe46WlpWaNiAj7qVNVVWVmYmJizIyLQCDgOV5SUmLWaNy4sZm59NJLnefkxeU5UVRUFJJjAZbo6GgzU11dbWaCwaCZadCggZmZPHmymbEsW7bMzLRr187MuNwma02VpJ07d5oZwIX1vM3OzjZruDxnt23bZmZGjhwZkjpxcXGe4w0bNjRr5Ofn1/k4khQVFWVmLC77n9jY2JDUcdkrNGvWzMxYj9P3339v1ujatauZmT17tplx2bOtX7/ezLRo0cJz/LvvvjNr4MDn8nyznrdHHnmkWcPlteqy/pSXl5sZa18XGRlp1nA5V3E5H6ysrDQzHTp0MDOffPKJ57jL4whIdv/Beu+QpPnz55uZ4uJiM9O9e3cz47IH+vTTTz3Hk5OTzRqpqalmxmUdc1k7LFu3bjUzLueUAwcONDM//PCDmXHpsVn9KJfbtGXLFjPjsqa6PE47duwwM02aNDEz+wpXXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAVyLqewJ/BWFh9s8HqqurzUzPnj3NTHJyspmprKz0HE9KSjJrlJaWmpm4uDgzExUVZWbKy8vNTFlZmed4VVVVnWtI0ssvv2xmXLg83tZ9EwgEzBoutwmoqKgwMy7rWEJCQiimo++++67ONTZs2BCCmbjdbhfZ2dkhqQM0bdrUc9x6j5fc3hMPPvhgM7N161Yzc+GFF5qZgQMHeo4vWbLErOFym8LDw81MKITqOC77H5d935YtW8xM7969PcffeOMNs0Zubq6Z6dChg5nJyckxMy7Pc2vf/O6775o1ABfR0dFmxuXcqbCw0MxERkaaGescwuUczWX/k5qaamZWr14dkmMNGjTIc/zNN980awCSlJmZ6Tm+fPlys4bLnqOoqMjM/Pjjj2YmLS3NzEREeLf1UlJSzBout9tl7fjyyy/NzAknnOA53rhxY7OGyz7Ahct9s3nzZjPjct9YXPbVLs89l72hy/vNMcccY2b2Fa64BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICvRNT3BA4EYWHe/f/KysqQHOf66683M+Hh4WYmEAh4jldXV5s1IiLsp451v7jMRZIKCgrMzM8//+w53qdPH7PGzJkzzcy+VF5eXt9TwF/Ejh07QlInGAyGpM7GjRvrXCMnJycEM3Fbo1ysWrUqJHWA1NRUz/HCwkKzRnR0tJlp166dmXn66afNTGJiYp0zZWVlZo2oqCgzU1VVZWZcWHstl/dvl/2ay/4xKSnJzLjcfy1atPAcf/bZZ80aXbp0MTMdO3Y0M59++qmZWb16tZk58sgjzQwObC7v4S57F+u8JyYmxqyxffv2fTIXSYqNjfUc37Ztm1nD5TYlJCSYGRcu+1CX81PAxciRIz3HXdaN5ORkM+Pyeu7QoYOZCcVz32Vv6LJfS0lJMTMnnniimbH2NyUlJWaNZs2amZmtW7eaGWu9lNz2ddZe65BDDjFrNGnSxMxkZ2ebmby8PDPjsm9evHix53haWppZY8WKFWbGBVdcAwAAAAAAAAB8hcY1AAAAAAAAAMBXaFwDAAAAAAAAAHyFxjUAAAAAAAAAwFdoXAMAAAAAAAAAfIXGNQAAAAAAAADAV2hcAwAAAAAAAAB8JaK+J3AgCAvz7v9XV1ebNXr37m1mGjVqZGbKysrMTEJCgue4y3zDw8PNTGlpqZkpLy83MyUlJWbG0rhxYzOzZMmSOh8nlPr37+85PmbMGLPG3LlzQzUdHMC2b98ekjou64KLVatW+aKGJEVFRZmZiooKM7Nz585QTAdQamqq53hOTo5Zo6ioyMwkJyebmY0bN5qZDh06mBlr7+KyV0hMTDQzxcXFZqaqqsrMREdHe467zDdUXPZ9LrcpOzvbc7x58+Z1riFJXbp0MTPW/keS1qxZY2aAYDAYkjqxsbGe45WVlSE5TlxcnJlx2XNY53ouexKX9wCX2x0RYbcbXPaPBQUFZgZwYb1XBQIBs8Z5551nZl577TUz43Ke4bKONW3a1HPc6le5Zlz2US69m5iYGM9xl96Ny7qxbds2M2Pdd5KUkpJiZnJzcz3Hd+zYEZLjuLD2qZLUsGFDM5OXl+c53rlzZ7PGihUrzIwLrrgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK9E1PcE9pawMLsnX11dHZJjVVZWeo4HAgGzxlNPPWVmtm7dambCw8PNTFlZWZ1rWLfZtY7LYxAMBs1M+/btPceTkpLMGpMmTTIzLm666SYzc9FFF5mZiAjvl2dycrJZY86cOWYGyMnJCUmdyMjIkNTZsGFDnWusW7fOzFRVVZmZmJgYM5OVleU0JyAUEhMTPcddnvvbt283M3369HGek5emTZuamejoaM9xl9eq9Z4pSXFxcWYmNzfXzIRCqG6TtaeT7PtXkgoKCjzHzz77bLPG//3f/5kZl9t08MEHm5mVK1eamfz8fM/xqKgos0Z5ebmZwYHP2gu4vA5dnksua9SOHTvMTEJCgud4RUWFWSM1NdXMuLyeXY7lstdyed8CXNxxxx11rnH99debGZfeQuPGjc3M6tWrzcxXX33lOe7yfte2bduQzMWl72KtdYWFhWYNa52T3HpWsbGxZsbaI0l2X8ulN+aSadGihZlxWS8vvvhiM2OdU7j06UKFK64BAAAAAAAAAL5C4xoAAAAAAAAA4Cs0rgEAAAAAAAAAvkLjGgAAAAAAAADgKzSuAQAAAAAAAAC+QuMaAAAAAAAAAOArNK4BAAAAAAAAAL5C4xoAAAAAAAAA4CsR9T2BvaW6ujokdSIi7LuosrLSc7xTp05mjVGjRpmZu+66y8w0bNjQzOTl5ZkZi8v9GxkZaWZc7l+X2xQXF+c5vmHDBrPG3/72NzNz3333mZnCwkIzU1JSYmZyc3M9x63nnSStW7fOzAArV66s7ynUUlxcXOcaLq8xF4FAwMwUFRWF5FiAi+joaM9xl/fn5ORkM5Odne06JU+HHHKImdm6davneGlpqVnD2ge42rZtm5kJDw+v83HKy8tDchzr+SC5ranr16/3HO/atatZw8WCBQvMjMtjUFBQYGas252YmGjW2L59u5nBgS8pKclzPCoqyqwRHx9vZioqKsyMy7pgnYMFg0GzRuPGjc2MtXZLUkxMjJlxOTdNSEgwM8C+kp+fb2YGDx5sZvr3729mnn76aTNj7bVc9gEutykszL7utVWrVmZm1apVnuMu+z6XtcXlfNBlPXTpWVn7MZdzSpfHYNy4cWbmww8/NDP7G664BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICvRNT3BOpTWJjdt6+srDQz0dHRnuNnnXWWWePbb781MzfffLOZef75581MVVWV57h1e1xqSFJUVJSZSUpKCsmxCgoKPMfz8/PNGhMmTDAz//d//2dmMjIyzEzfvn3NTGlpqee4y/1r1QAkaenSpSGpEwwGzYzL6zkUXNbuQCAQkmPt2LEjJHUAF6mpqZ7jO3fuNGs0bNjQzOTk5DjPyUu7du3MTHl5ued4RIS9XQ0PDzczLuuPy9phzcfl/bmkpMTMuNRxuU0JCQlmprCw0HPc5X5xsXXrVjOTnJwckmO1aNHCc7xVq1Zmje3bt4dkLti/NWjQwHPcOg+RpIqKCjPjcm5qrZeS/Rpq2rSpWaNRo0ZmZtmyZWYmVGuHtR5GRkaaNVweAxz4rP2/y/mByzmPS8blNeTSm7EyeXl5Zg0XLnNZsmSJmbHWVJc9ncta2KNHDzPjssd0eSyzs7M9x13WXZd17MMPPzQzByKuuAYAAAAAAAAA+AqNawAAAAAAAACAr9C4BgAAAAAAAAD4Co1rAAAAAAAAAICv0LgGAAAAAAAAAPgKjWsAAAAAAAAAgK/QuAYAAAAAAAAA+AqNawAAAAAAAACAr0TU9wT2lvDwcDNTVVUVkmNdddVVnuN9+/Y1awwZMsTMrFy50szs2LHDzLRt27bOxwkEAmYmISHBzERFRZmZjRs3mpnS0lLP8R49epg1Tj31VDPz/vvvm5n33nvPzISF2T8zqqio8Bw/+OCDzRqVlZVmBti+fXtI6kRGRpoZl7U5FFJTU82MyzrmoqCgICR1AJf3xHbt2nmOFxcXmzVatGhhZkL1vE5PTzczn3/+ued4YmKiWWNfrS2SvdZFRNjba5f3Z5c6LrfbZW2Oi4vzHM/PzzdrdOvWzcysXbvWzHTp0sXMuNx/1j6/cePGZg34l8t7eDAYDMmxrNdHeXm5WcNlvg0bNjQz2dnZITlWKOYSHR1tZvbV3jA5OdnMbN26tc7Hwf7PWhdc1o1Qvc83adLEzLiw3u9c9pexsbFmxuU25ebmmhlLUlKSmXHpp7jMxWUvsGbNGjOTkvL/a9duYuyqyz+AnzvvnXbodNrSF6aFUaDYoNQGEqQQIcaXoETdQFiQKDEm7l0YEjdu3JmoMWxcGBOXsDBGjRoNITEGjUrBWqBGcNpSaIdO5/3lzlxX/43J/3x/yRymx/r5bJ9vnnPunXN+53eeuRO19eXl5dijZB37X+UX1wAAAAAAtIrBNQAAAAAArWJwDQAAAABAqxhcAwAAAADQKgbXAAAAAAC0isE1AAAAAACtYnANAAAAAECrGFwDAAAAANAqA9f7BN4vfX15Jr+xsREzX/nKV2LmxIkTtfX+/v7YY3Z2Nma+8IUvxMwnP/nJmHnsscdq6w8++GDsMTc3FzM333xzzLz99tsxs7CwEDOnTp2qrT/wwAOxxx/+8IeYKXHo0KGYWV9fj5lOp7OlelVV1erqaszA8vJyI32GhoYa6dOEkvWnxMBAfkx2u91GjgXDw8Mx08S6Pj4+HjNLS0tbPk5VVdXBgwdj5vLly7X1ku+lZE+3srISM5ubm41kkrW1tZgp+Vvv2rVry+dSVXnfXLJP3b9/f8z861//ipn77rsvZkpcuHChtl6yP6e9er1eI3327t0bMzt27Kitl9wfIyMjMXPHHXfEzOjoaMxcuXKltl5yj73++usxc+7cuZgp2Rteu3YtZvbs2VNb37lzZ+yRnjVQquQ9vMR7770XMyVrXZqplMzGStaxks998uTJLR+r5N205J4fGxuLmZLvpmS+MzMzU1svOd+zZ8/GTFNK/pZNPWeb4BfXAAAAAAC0isE1AAAAAACtYnANAAAAAECrGFwDAAAAANAqBtcAAAAAALSKwTUAAAAAAK1icA0AAAAAQKsMXO8T+E+dTidm+vryvH19fb2J06keeuihmLn33ntr62+++WYjx/ne974XM3/+859j5gc/+EHMJIcOHYqZd999N2YWFhZi5tSpUzFz4sSJ2vrp06djj6bs2rUrZjY2NmKm5F5Irl27tuUeUFVl1+zg4GDMvP32202cTlTyDCi5x0o+EzTlwIEDMTM/P19bHxjIW7uSzNmzZ2Om5HyXlpZiJq0LJccZGxuLmUuXLsXM3r17Yyath/v27Ys9Sr6XtbW1mCkxPj4eM++8805t/aabboo9RkdHY2Zubi5mVldXY2Z4eHjLmampqdiDG1/Je2W6Py5fvhx7lKwtv//972Pmsccei5m0xr/wwguxx/Hjx2PmnnvuiZmSdezFF1+MmfQ+3e12Yw9oSq/Xa6RPyfpTsr5MTk5u+Vz++Mc/xkzJ/nH//v0xk57zO3bsiD1eeumlmHnkkUdiZnFxMWaa2JeU7PtK5kj/q/ziGgAAAACAVjG4BgAAAACgVQyuAQAAAABoFYNrAAAAAABaxeAaAAAAAIBWMbgGAAAAAKBVDK4BAAAAAGgVg2sAAAAAAFpl4HqfwH/q9Xoxs7GxETNjY2Mx8+Uvfzlmvv71r8fM97///dr6Rz7ykdjjr3/9a8w888wzMfPzn/88Zm655Zba+uzsbOyxtrYWMwsLCzGzZ8+emPnsZz8bM6dPn66t9/Xl/9Fsbm7GTImSz7S8vBwznU6ntj4wkG/fkvsJSpTczyX3WX9/f8wMDg7W1tfX1xs5l5J7vuR8V1dXYwZKDA8Px0zaA5Vcs+Pj4zHz29/+NmaOHz8eM2+88UbMlJxzUvLsLdm7lPRJa1TJPjXtxaqqqubm5mKmqb3L6Ohobf2f//xn7HH06NGYKbkeSj53yd8prc0l9xvtlfbJVVW2Dx4aGoqZJ554orZ+7ty52GP//v0xU7IWTk5Oxswrr7xSWy/Zt3z+85+PmTNnzsRMyWe68847Yyat33/5y19ij0uXLsUMbKeS51B6PldVVXW73dp6yXq5srISMw899FDMpD1SVeU1qGQ2VrJGlexlP/jBD8bM9PT0lvuU7EFL/gZNaeoZul384hoAAAAAgFYxuAYAAAAAoFUMrgEAAAAAaBWDawAAAAAAWsXgGgAAAACAVjG4BgAAAACgVQyuAQAAAABoFYNrAAAAAABaZaDJZp1OJ2Z6vd6Wj3PvvffGzLe+9a2YOX36dMw8/vjjMfPrX/+6tv6hD30o9njqqadi5umnn46Zj33sYzHz4x//uLb+4IMPxh5LS0sxMz4+HjMvvPBCzPziF7+Ime1S8plmZmZiZnBwsIGzyTY3N7flONz43nnnnZg5dOhQzAwM5MdOus8uX74ce3S73ZgpeR6VPNeuXbsWM1DiyJEjMfPee+/V1ufn52OP2dnZmCl5Tt1///0xc+7cuZhpwtWrV2Om5LvZt29fzKyvr9fWS/YBR48ejZkSq6urMVPyuZM33ngjZk6ePBkzH/3oR2NmZGSk6JySixcv1tZL7jfaq4l3yqqqqgsXLsTMiy++WFsvua4nJydjZmJiImb6+/tjJq3xH/7wh2OPxcXFmCl5TuzYsSNmdu/eveXzmZubiz2gbUruj507d8ZMuhdLZgK33HJLzJS8D956660xk87n2LFjscfy8nLMlOy1brvttpgpmQGdP3++tl7ynrydmnqGbhe/uAYAAAAAoFUMrgEAAAAAaBWDawAAAAAAWsXgGgAAAACAVjG4BgAAAACgVQyuAQAAAABoFYNrAAAAAABaxeAaAAAAAIBWGbjeJ/Cf9u3bFzPf/OY3Y+aZZ56Jmb/97W8x8+1vfztmvvjFL9bWx8bGYo8vfelLMfOZz3wmZqanp2PmkUceqa13Op3Yo9vtxszw8HDMfPWrX42ZJpR8phIHDhyImdHR0ZhZWlqKmc3Nzdp6r9eLPZr63HDmzJmYOXz4cMz09/fHzNTUVG398uXLscfy8nLMlNxDJWZnZxvpA/fdd9+We9x1110xU7LXevjhh2Om5Hk3MzMTM0nJs3d+fj5mhoaGYqZkHbt48WJtfWNjI/YoyZQo+VuW7MfSd3Pp0qXYo2Rv+IEPfCBmSoyPj8fMkSNHausTExONnAs3vl/+8pdbqm+3Q4cO1davXr0ae/zwhz9s6nTghtPUM3xkZCRmSt7nFxcXa+sl7yq33nprzJQ859fX12MmnU9aw0rPZXBwMGZ+85vfxMz+/ftjJu0f02ynqqqqr2/7fldc8k5e8h1vF7+4BgAAAACgVQyuAQAAAABoFYNrAAAAAABaxeAaAAAAAIBWMbgGAAAAAKBVDK4BAAAAAGgVg2sAAAAAAFrF4BoAAAAAgFYZaLJZr9fbco8rV67EzN///veYefrpp2Pm6NGjMTM6Ohozr776am19fHw89njyySdjpr+/P2bW1tZiZnBwsLY+MzMTe5T49Kc/3UifdL5VVVXr6+u19YGBfKlvbGzEzL59+2KmRMmxOp1ObX16ejr2KLkeoMS5c+di5tFHH42Z1dXVmJmamqqtv/TSS7HHyspKzGxubsZMX1/+/27J/QwlfvSjH8VMuj+Gh4djj+eeey5mTp48GTMHDhyImdnZ2ZhJlpaWYqZkj3Tw4MEtn0tV5XVhcXEx9ihZo/bs2RMz8/PzMVOyd0n775LrquRvXfJ3ev7552Om5FmSvpuSHlBV+Z4v2U+0Scn6A7z/SvYlCwsLMZPWqJJneLfbjZmSmcrExETMjI2N1dZL5lElc46Sz/3AAw/EzJ/+9KeYOXPmTG394x//eOxx0003xUzJu2nJM+m/7bnlF9cAAAAAALSKwTUAAAAAAK1icA0AAAAAQKsYXAMAAAAA0CoG1wAAAAAAtIrBNQAAAAAArWJwDQAAAABAqxhcAwAAAADQKgNNNjt27FjMfOMb36itX716NfY4depUzBw+fDhm5ubmYmZtbS1mNjY2aut9ffn/AyXHGR0djZnV1dWYuXbtWm19586dscezzz4bM6+99lrM9Pf3x0y3242ZpORvUGJkZCRmer1ezKRrpiSzubkZe5ScL5R49dVXY6ZkHSu5nw8ePFh0TnWGhoZipuReLdHU+gLT09ONZJowNjYWM/fff3/MLC0txUzaC5SsLSX3fMmeY3Z2NmbSs3XPnj2NHOfuu++OmZdffjlmSvbE6Tsu+UwXLlyImfn5+Zj52c9+FjOwnUr23NtlYCC/vqd9ya5du2KPhYWFmOl0OjHT1F4L2mR4eDhmVlZWYuaee+6JmZJ3pytXrtTWp6amYo+m7ueSzO7du2vre/fujT1K5invvvtuzJTsH0v2u8vLy7X1kn1fyRyuZE93/vz5mPlvW5u9bQMAAAAA0CoG1wAAAAAAtIrBNQAAAAAArWJwDQAAAABAqxhcAwAAAADQKgbXAAAAAAC0isE1AAAAAACtMtBks9deey1mfvKTn9TW77jjjthjfn4+ZiYmJmJmx44dMXPnnXfGzJEjR2rrc3NzsUev14uZ2dnZmFlZWYmZgwcP1ta/853vxB7f/e53Y6bE5uZmzJR8N00cp8TDDz8cM3v37o2ZbrcbM+n6LLk2h4aGYgZKvPLKKzFTsqbu2rUrZj7xiU/U1kvWn7W1tZhZWFiImbGxsZjpdDoxA/9tPve5z8VMf39/I8dKz820z6qqsnt+dHS0+JzebyV72bvuuquRY91+++0xc/r06dr6+Ph47FGyBy35TF/72tdi5tlnn42ZdH1ubGzEHtA2Je8QS0tLtfXh4eHYo2SP1MQ7Gmy3tG/fzuv62LFjMTM1NRUzy8vLtfWSe/7ll1+OmZJ9VMncZXJysra+e/fu2OOtt96KmbQWVlVVDQzkkWjJbDHt60o+09133x0z6burqqo6f/58zJTMiVZXV2Nmu/jFNQAAAAAArWJwDQAAAABAqxhcAwAAAADQKgbXAAAAAAC0isE1AAAAAACtYnANAAAAAECrGFwDAAAAANAqBtcAAAAAALRKp9fr9YqCnc77fS43rImJiZiZnJxs5FhXrlyJmYsXLzZyrKTkmim8/LZsYGAgZrrdbsycOHEiZqampmLmH//4R8ysrq7W1ku+37Nnz8bMdv0N3m/WqOvv+eefj5lf/epXMfPTn/60tt7UGjYyMhIzo6OjMbO4uBgz6X7m/2eNuj5uv/32mDl58mTMvP766zHz5ptvbvlcSp7hJRYWFmImrQsle7ESJfuJkrXlyJEjMXPmzJmic9qq48ePx8zs7GzM/O53v9vyufT398fMxsZGzFij2E5DQ0Mxk949Z2ZmYo+SNfVGufZvdDfK36lNa1RfX/795+bmZswcOHAgZj71qU/FTJp1rK+vb7lHVVXV9PR0zBw+fDhm0vO3ZG9T8h5X8ncaHByMmbfeeitmbr755tp6yfWwtLQUM88991zMlGhqPtaEkjXKL64BAAAAAGgVg2sAAAAAAFrF4BoAAAAAgFYxuAYAAAAAoFUMrgEAAAAAaBWDawAAAAAAWsXgGgAAAACAVjG4BgAAAACgVTq9Xq93vU8CAAAAAAD+j19cAwAAAADQKgbXAAAAAAC0isE1AAAAAACtYnANAAAAAECrGFwDAAAAANAqBtcAAAAAALSKwTUAAAAAAK1icA0AAAAAQKsYXAMAAAAA0Cr/Bj4kbI1azlUSAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1. PCA con Autoencoder Lineal"
      ],
      "metadata": {
        "id": "to5LGrlgs5OT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA con los vectores achatados**\n",
        "\n",
        "*Repaso de PCA.* PCA es una técnica de reducción de dimensionalidad para simplificar conjuntos de datos sin perder demasiada información relevante. El objetivo es transformar las variables originales en un nuevo conjunto de variables ortogonales: las componentes principales, que capturan la máxima varianza posible en los datos con la menor cantidad de dimensiones.\n",
        "Formalmente, el objetivo de PCA es encontrar una matriz de proyección $\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$ (con $k \\leq d$) tal que las nuevas variables $\\mathbf{Z} = \\mathbf{X} \\mathbf{W}$ conserven la mayor varianza posible.\n",
        "\n",
        "Se expresa como el siguiente problema de optimización:\n",
        "$$\n",
        "\\max_{\\mathbf{W} \\in \\mathbb{R}^{d \\times k}} \\; \\operatorname{Tr} \\left( \\mathbf{W}^\\top \\mathbf{S} \\mathbf{W} \\right) \\quad \\text{sujeto a} \\quad \\mathbf{W}^\\top \\mathbf{W} = \\mathbf{I}_k\n",
        "$$\n",
        "donde:\n",
        "\n",
        "- $\\mathbf{S} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X}$ es la matriz de covarianza estimada de los datos,\n",
        "- $\\operatorname{Tr}(\\cdot)$ denota la traza de una matriz,\n",
        "- $\\mathbf{I}_k$ es la matriz identidad de tamaño $k$.\n",
        "\n",
        "La solución óptima está dada por las $k$ autovectores de $\\mathbf{S}$ correspondientes a los $k$ autovalores más grandes. Estos vectores definen las direcciones principales en las que los datos varían más.\n",
        "\n",
        "A partir de un subconjunto de imágenes achatadas, realizarás los siguientes:\n",
        "\n",
        "1.\tConstruí la matriz de datos.\n",
        "Extraé un batch de imágenes del conjunto de entrenamiento y convertilo en una matriz de tamaño (N, 784), donde cada fila representa una imagen de 28x28 píxeles aplanada.\n",
        "2.\tAjustá el modelo PCA. Aplicá PCA con 64 componentes principales y ajustalo a los datos.\n",
        "3.\tGraficá la varianza acumulada explicada en función del número de componentes principales.\n",
        "4. Visualizá las primeras 10 componentes principales como imágenes de 28x28 píxeles.\n",
        "5.\tReconstrucción. Tomá un subconjunto de 10 imágenes, proyectalas en las componentes principales y reconstruílas usando la transformación inversa. Compará las imágenes originales y las reconstruidas.\n"
      ],
      "metadata": {
        "id": "am9Ff2opu4-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) TODO\n",
        "X =   # shape (256,784)\n",
        "# 2) fit PCA\n",
        "pca = PCA(n_components=64)\n",
        "pca.fit(X)\n",
        "\n",
        "# 3)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
        "plt.xlabel('componentes'); plt.ylabel('varianza explicada cumulativa(%)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f6Th0nZeuWgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los primeros 10 PCs\n",
        "pcs = pca.components_[:10].reshape(-1,28,28)\n",
        "fig, axs = plt.subplots(2,5,figsize=(10,4))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "    ax.imshow(pcs[i], cmap='gray'); ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hoVch-AnvcGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X[:10]\n",
        "X_rec = pca.inverse_transform(pca.transform(X_test))\n",
        "fig,axs = plt.subplots(2,10,figsize=(15,3))\n",
        "for i in range(10):\n",
        "    axs[0,i].imshow(X_test[i].reshape(28,28), cmap='gray'); axs[0,i].axis('off')\n",
        "    axs[1,i].imshow(X_rec[i].reshape(28,28), cmap='gray'); axs[1,i].axis('off')\n",
        "plt.suptitle('arriba: original — abajo: reconstruccion PCA '); plt.show()"
      ],
      "metadata": {
        "id": "UqS_jkn3vemb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Con Autoencoder lineal sin función de activación**\n",
        "\n",
        "En este ejercicio entrenarás un autoencoder lineal sobre el mismo dataset. El modelo consta de una red con arquitectura simétrica:\n",
        "\n",
        "*\tEncoder: una única capa lineal que proyecta la imagen a un espacio latente de dimensión $d$.\n",
        "*\tDecoder: una única capa lineal seguida de una función sigmoide, que proyecta el espacio latente de vuelta al espacio de imágenes.\n",
        "\n",
        "El código entrena este modelo usando descenso por gradiente estocástico (`SGD`) y error cuadrático medio (`MSE`) como función de pérdida."
      ],
      "metadata": {
        "id": "MwNU2yRHvqhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIONES para LinearAE\n",
        "LATENT_DIM=10\n",
        "LR=0.5\n",
        "MAX_EPOCH=20"
      ],
      "metadata": {
        "id": "yhqbH76pR6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, latent_dim=2):\n",
        "        super().__init__()\n",
        "        # encoder = Linear(784 a latent_dim)\n",
        "        self.encoder = # TODO\n",
        "        # decoder = Linear(latent_dim a 784) + sigmoid para que los valores de los pixeles estén en [0,1]\n",
        "        self.decoder = # TODO\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        return xrec, z\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model  = Autoencoder(latent_dim=LATENT_DIM).to(device)\n",
        "opt    = optim.SGD(model.parameters(), lr=LR)\n",
        "crit   = nn.MSELoss()\n",
        "\n",
        "# 4) Train\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, _ in train_loader:\n",
        "       # TODO\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d}  loss={total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "JMFV2cdSvvlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usá el modelo entrenado para obtener los vectores latentes $\\mathbf{z} \\in \\mathbb{R}^2$ de un batch de imágenes del conjunto de test. Creá un scatter plot donde cada punto representa una imagen codificada por el autoencoder en el espacio latente. Usá el color para indicar la etiqueta de clase de cada imagen."
      ],
      "metadata": {
        "id": "zqVTmwDHSsH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "xb, yb = next(iter(test_loader))\n",
        "xb = xb.to(device)\n",
        "with torch.no_grad():\n",
        "    _, z = model(xb)\n",
        "z = z.cpu().numpy()\n",
        "yb = yb.numpy()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "scatter = plt.scatter(z[:,0], z[:,1], c=yb, cmap='tab10', s=5)\n",
        "plt.legend(*scatter.legend_elements(), title=\"class\", loc=\"upper right\")\n",
        "plt.title(\"Espacio Latente 2D de Fashion-MNIST\")\n",
        "plt.xlabel(\"z0\"); plt.ylabel(\"z1\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zmA_rRWHwzFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compará los resultados entre PCA y el Autoencoder Lineal, tomando un batch del test_loader. ¿Qué observa?"
      ],
      "metadata": {
        "id": "MYIFp0g0TFYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Reconstrucción\n",
        "# Comparación entre PCA y Autoencoder\n",
        "\n",
        "# Obtener datos de prueba\n",
        "xb, _ = next(iter(test_loader))\n",
        "xb = xb.to(device)\n",
        "X_test = xb.cpu().numpy()\n",
        "\n",
        "# Reconstrucción con PCA\n",
        "# Usar solo las primeras 10 componentes principales\n",
        "X_pca_10 = pca.transform(X_test)[:, :10]  # (B, 10)\n",
        "# Redimensionar a (B, 64) rellenando con ceros\n",
        "X_pca_10_pad = np.zeros((X_pca_10.shape[0], 64))\n",
        "X_pca_10_pad[:, :10] = X_pca_10\n",
        "# Reconstrucción desde solo 10 PCs\n",
        "X_pca_rec_10 = pca.inverse_transform(X_pca_10_pad)\n",
        "\n",
        "# Reconstrucción con Autoencoder\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xr, _ = model(xb)\n",
        "X_ae_rec = xr.cpu().numpy()\n",
        "\n",
        "# Visualización\n",
        "n = 10\n",
        "fig, axs = plt.subplots(3, n, figsize=(15, 5))\n",
        "for i in range(n):\n",
        "    axs[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(X_pca_rec_10[i].reshape(28, 28), cmap='gray')\n",
        "    axs[1, i].axis('off')\n",
        "    axs[2, i].imshow(X_ae_rec[i].reshape(28, 28), cmap='gray')\n",
        "    axs[2, i].axis('off')\n",
        "axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n",
        "axs[1, 0].set_ylabel(\"PCA\", fontsize=12)\n",
        "axs[2, 0].set_ylabel(\"Autoencoder\", fontsize=12)\n",
        "plt.suptitle(\"Reconstrucción: Original vs PCA vs Autoencoder\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# MSE cuantitativo\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse_pca = mean_squared_error(X_test, X_pca_rec_10)\n",
        "mse_ae  = mean_squared_error(X_test, X_ae_rec)\n",
        "\n",
        "print(f\"MSE reconstrucción PCA:         {mse_pca:.4f}\")\n",
        "print(f\"MSE reconstrucción Autoencoder: {mse_ae:.4f}\")"
      ],
      "metadata": {
        "id": "xJapbUoDyNHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2. Deep Autoencoder\n",
        "\n",
        "¿Qué ocurre si ahora agregamos más capas densas y funciones de activación tanto para el `encoder` como para el `decoder`?\n",
        "\n",
        "Experimentar con diferentes arquitecturas, dimensión del vector latente, optimizadores, etc. Por ejemplo, 2 capas densas con ReLU con Adam.\n",
        "\n",
        "El procedimiento para entrenar sigue igual."
      ],
      "metadata": {
        "id": "Bz_w8P51kmYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIONES para DeepAE\n",
        "LATENT_DIM=10\n",
        "LR=1e-3\n",
        "MAX_EPOCH=20"
      ],
      "metadata": {
        "id": "3jPuvKXjUxFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, latent_dim=10):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO\n",
        "        return xrec, z\n",
        "\n",
        "model_deep = DeepAutoencoder(latent_dim=LATENT_DIM).to(device)\n",
        "opt = optim.Adam(model_deep.parameters(), lr=LR)\n",
        "crit = nn.MSELoss()\n",
        "\n",
        "# Train\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model_deep.train()\n",
        "    total_loss = 0\n",
        "    for xb, _ in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        # TODO\n",
        "    print(f\"[Deep] Epoch {epoch+1:2d}  loss={total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "DsMz8bjckl-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstrucción visual vs Autoencoder lineal\n",
        "model_deep.eval()\n",
        "xb, _ = next(iter(test_loader))\n",
        "xb = xb.to(device)\n",
        "X_test = xb.cpu().numpy()\n",
        "\n",
        "# Forward pass en deepAE\n",
        "with torch.no_grad():\n",
        "    xr_deep, _ = model_deep(xb)\n",
        "X_deep_rec = xr_deep.cpu().numpy()\n",
        "\n",
        "# Forward pass en LinearAE\n",
        "with torch.no_grad():\n",
        "    xr_linear, _ = model(xb)\n",
        "X_linear_rec = xr_linear.cpu().numpy()\n",
        "\n",
        "n = 10\n",
        "fig, axs = plt.subplots(3, n, figsize=(15, 6))\n",
        "for i in range(n):\n",
        "    axs[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    axs[1, i].imshow(X_ae_rec[i].reshape(28, 28), cmap='gray')\n",
        "    axs[2, i].imshow(X_deep_rec[i].reshape(28, 28), cmap='gray')\n",
        "    for row in range(3):\n",
        "        axs[row, i].set_xticks([])\n",
        "        axs[row, i].set_yticks([])\n",
        "        axs[row, i].spines[:].set_visible(False)\n",
        "\n",
        "axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n",
        "axs[1, 0].set_ylabel(\"Linear AE\", fontsize=12)\n",
        "axs[2, 0].set_ylabel(\"Deep AE\", fontsize=12)\n",
        "\n",
        "plt.suptitle(\"Reconstrucción: Original vs PCA vs Linear AE vs Deep Autoencoders\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "mse_deep = mean_squared_error(X_test, X_deep_rec)\n",
        "print(f\"MSE reconstrucción Deep Autoencoder: {mse_deep:.4f}\")"
      ],
      "metadata": {
        "id": "TA6n8hoz1QxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.3. [Opcional] Denoising Autoencoder\n",
        "\n",
        "Modificá el entrenamiento del autoencoder profundo para que funcione como un Denoising Autoencoder. Esto implica:\n",
        "\n",
        "1.\tAgregar ruido gaussiano a las imágenes de entrada durante el entrenamiento.\n",
        "2.\tEntrenar el autoencoder para que reconstruya la imagen original limpia a partir de su versión ruidosa.\n",
        "\n",
        "Instrucciones:\n",
        "- Usá ruido aditivo gaussiano: `x_noisy = x + noise`, con `noise `$\\sim \\mathcal{N}(0, \\sigma^2)$ (por ejemplo, $\\sigma^2 = 0.3$).\n",
        "- Durante el entrenamiento, el input del modelo debe ser `x_noisy`, pero la pérdida debe calcularse respecto a `x` (sin ruido).\n",
        "- Evaluá la reconstrucción visualmente comparando la imagen original, la ruidosa y la reconstruida."
      ],
      "metadata": {
        "id": "Jr1FgYVDw3PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIONES para DnoiseAE\n",
        "LATENT_DIM=10\n",
        "LR=1e-3\n",
        "MAX_EPOCH=20\n",
        "NOISE_FACTOR=0.3"
      ],
      "metadata": {
        "id": "OoTF9dwsY3_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, latent_dim=10):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        return xrec, z\n",
        "\n",
        "    @staticmethod\n",
        "    def add_noise(x, noise_factor=0.3):\n",
        "      # TODO\n",
        "      # Asegurarse que el output esté entre 0,1\n",
        "\n",
        "model_denoise = DenoisingAutoencoder(latent_dim=LATENT_DIM).to(device)\n",
        "opt = optim.Adam(model_denoise.parameters(), lr=LR)\n",
        "\n",
        "# Train with noise\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    model_denoise.train()\n",
        "    total_loss = 0\n",
        "    for xb, _ in train_loader:\n",
        "       # TODO\n",
        "    print(f\"[Denoising] Epoch {epoch+1:2d}  loss={total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "gH3Yc9jhw2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstrucción visual DenoiseAE vs Deep AE\n",
        "xb, _ = next(iter(test_loader))\n",
        "xb = xb.to(device)\n",
        "X_test = xb.cpu().numpy()\n",
        "\n",
        "# Forward pass en DenoiseAE\n",
        "model_denoise.eval()\n",
        "with torch.no_grad():\n",
        "    xb_noisy = model_denoise.add_noise(xb)\n",
        "    xr_denoise, _ = model_denoise(xb_noisy)\n",
        "X_denoise_rec = xr_denoise.cpu().numpy()\n",
        "\n",
        "# Foward pass en DeepAE\n",
        "with torch.no_grad():\n",
        "    xr_deep, _ = model_deep(xb)\n",
        "X_deep_rec = xr_deep.cpu().numpy()\n",
        "\n",
        "fig, axs = plt.subplots(4, n, figsize=(15, 8))\n",
        "for i in range(n):\n",
        "    axs[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    axs[1, i].imshow(X_deep_rec[i].reshape(28, 28), cmap='gray')\n",
        "    axs[2, i].imshow(xb_noisy[i].cpu().numpy().reshape(28,28), cmap='gray')\n",
        "    axs[3, i].imshow(X_denoise_rec[i].reshape(28, 28), cmap='gray')\n",
        "    for row in range(4):\n",
        "        axs[row, i].set_xticks([])\n",
        "        axs[row, i].set_yticks([])\n",
        "        axs[row, i].spines[:].set_visible(False)\n",
        "\n",
        "axs[0, 0].set_ylabel(\"Original\")\n",
        "axs[1, 0].set_ylabel(\"Deep AE\")\n",
        "axs[2, 0].set_ylabel(\"Noisy input\")\n",
        "axs[3, 0].set_ylabel(\"Denoise AE\")\n",
        "\n",
        "plt.suptitle(\"Reconstrucción con Denoising Autoencoder\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "mse_deep = mean_squared_error(X_test, X_deep_rec)\n",
        "print(f\"MSE reconstrucción Deep Autoencoder: {mse_deep:.4f}\")\n",
        "mse_denoise = mean_squared_error(X_test, X_denoise_rec)\n",
        "print(f\"MSE reconstrucción Denoising Autoencoder: {mse_denoise:.4f}\")"
      ],
      "metadata": {
        "id": "-p_pEn1d34yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. GANs\n",
        "\n",
        "### 2.1. Repaso teórico\n",
        "\n",
        "Las GANs son una clase de modelos generativos basados en juegos de suma cero entre dos redes neuronales: el generador y el discriminador.\n",
        "- Generador $G_\\theta(z)$: Toma como entrada un vector aleatorio $z \\sim p_z(z)$ (por ejemplo, una gaussiana estándar), y genera una muestra sintética $G_\\theta(z)$ que intenta imitar una muestra real $x \\sim p_{\\text{data}}$.\n",
        "- Discriminador $D_\\phi(x)$: Toma una muestra $x$ (real o generada) y predice la probabilidad de que provenga del conjunto de datos reales. Su objetivo es distinguir entre datos reales y falsos.\n",
        "\n",
        "El entrenamiento se modela como un juego minimax con la siguiente función objetivo:\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D ; V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "El discriminador intenta maximizar su capacidad de distinguir muestras reales de las generadas, maximizando la probabilidad de asignar correctamente las etiquetas reales (1 para datos reales, 0 para generados).\n",
        "\n",
        "El generador intenta “engañar” al discriminador, produciendo muestras tales que $D(G(z)) \\to 1$, es decir, que sean clasificadas como reales.\n",
        "\n",
        "*Observar* que la función de pérdida que usaremos para entrenar es la Binary Cross Entropy.\n",
        "\n",
        "### 2.2. Ejercicio"
      ],
      "metadata": {
        "id": "oVfshv4AfrEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1. GANs\n",
        "\n",
        "1. Cargar y visualizar el dataset\n",
        "2. Definir el Discriminador y el Generador\n",
        "3. Entrenar el GAN\n",
        "4. Visualizar las imágenes generadas"
      ],
      "metadata": {
        "id": "rn5Ub9dcox6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"spandan2/cats-faces-64x64-for-generative-models\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "D8ovdPHBuuwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CONFIGURACIONES GAN\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Usando {device}, sugerencia: usar T4 en colab para GPU')\n",
        "latent_dim = 100\n",
        "image_size = 64\n",
        "image_channels = 3\n",
        "batch_size = 256\n",
        "lr = 2e-4\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "l3jYPNJvknKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "transforms = transforms.Compose([transforms.Resize(image_size),\n",
        "                         transforms.CenterCrop(image_size),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize(*stats),\n",
        "                         transforms.Lambda(lambda x: x.view(-1))  # Aplano la imagen\n",
        "                        ])\n",
        "\n",
        "data = datasets.ImageFolder(\n",
        "    root= # TODO, completar con el path en la celda de kaggle\n",
        "    transform=transforms)\n",
        "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, num_workers=4, shuffle=True)"
      ],
      "metadata": {
        "id": "w2uXK7touyfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title veamos algunos ejemplos del dataset\n",
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]\n",
        "\n",
        "def show_images(images, nmax=64):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images, _ in dl:\n",
        "        show_images(images.view(-1, image_channels, image_size, image_size), nmax)\n",
        "        break\n",
        "\n",
        "show_batch(train_loader)\n"
      ],
      "metadata": {
        "id": "cBGgDzwGvY0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la siguiente arquitectura inicial para el Generador, pueden probar con otras.  3 capas lineales y cada una está seguida de una función de activación ReLU, excepto la última que usa Tanh. Para el discriminador, usaremos 3 capas lineales con activación LeakyReLU y la última capa sigmoidea (para retornar probabilidades).\n"
      ],
      "metadata": {
        "id": "8kF60R1fpr9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, hidden_dim=256, output_dim=image_channels*image_size*image_size):\n",
        "        super().__init__()\n",
        "        # TODO # Salida en [-1, 1]\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "G = Generator(latent_dim).to(device)"
      ],
      "metadata": {
        "id": "btjtF0RKlwF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=image_channels*image_size*image_size, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "D = Discriminator().to(device)"
      ],
      "metadata": {
        "id": "0XzVMNKXlzU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada epoch,\n",
        "1. Entrenamiento del Discriminador D\n",
        "\t1.\tTomar un mini-batch de imágenes reales del dataset: real_imgs\n",
        "\t2.\tGenerar un mini-batch de imágenes falsas con el generador.\n",
        "  3. Pasar imágenes reales por D y calcular función de pérdida para D.\n",
        "  4. Pasar imágenes falsas por D y calcular función de pérdida.\n",
        "  5. Sumar y backpropagar para actualizar los pesos en D.\n",
        "2. Entrenamiento al Generador G\n",
        "  1. Generar un nuevo mini-batch de ruido.\n",
        "  2. Generar imágenes falsas con G.\n",
        "  3. Pasar las imágenes falsas por D.\n",
        "  4. Calcular la función de pérdida del generador.\n",
        "  5. Actualizar los pesos de G."
      ],
      "metadata": {
        "id": "QRp6skYHqd5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar diferentes lr\n",
        "criterion = #TODO , qué loss usamos?\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=2e-4)\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "id": "MjzXR5_Bl12T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_noise = torch.randn(32, latent_dim).to(device)\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real_imgs, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # ======== Discriminador ========\n",
        "        # TODO\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ======== Generador ========\n",
        "        # TODO\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "      print(f\"Epoch {epoch+1}/{epochs} - Loss D: {loss_D.item():.4f} - Loss G: {loss_G.item():.4f}\")\n",
        "      # Mostrarmos las imágenes generadas\n",
        "      with torch.no_grad():\n",
        "          gen_imgs = G(fixed_noise).view(-1, image_channels, image_size, image_size)\n",
        "          gen_imgs = (gen_imgs + 1) / 2  # Desnormalizar\n",
        "          grid = vutils.make_grid(gen_imgs, nrow=8)\n",
        "          plt.figure(figsize=(8,8))\n",
        "          plt.axis(\"off\")\n",
        "          plt.title(f\"Imágenes generadas - Época {epoch+1}\")\n",
        "          plt.imshow(np.transpose(grid.cpu(), (1,2,0)))\n",
        "          plt.show()"
      ],
      "metadata": {
        "id": "qK9miVvPl61R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2. [Opcional] DCGAN\n",
        "\n",
        "Mejorar la calidad de las imágenes generadas usando una arquitectura más profunda basada en convoluciones.\n",
        "\n",
        "Sugerencia, para el generador usar ConvTranspose2d+BatchNorm+Relu y última capa con Tanh.\n",
        "Para el discriminador usar Conv2d + LeakyRelu y última capa sigmoid."
      ],
      "metadata": {
        "id": "_dcBP-olontC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGANGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class DCGANDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1)\n",
        "\n",
        "dcgan_transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),  # Normalizar entre [-1, 1]\n",
        "]) # el input es una imagen, por lo que no vamos a aplanarlo.\n",
        "\n",
        "dataset = # TODO\n",
        "dataloader = # TODO\n",
        "\n",
        "\n",
        "# Inicializar\n",
        "dcgan_G = DCGANGenerator().to(device)\n",
        "dcgan_D = DCGANDiscriminator().to(device)\n",
        "\n",
        "#\n",
        "criterion = # TODO\n",
        "optimizer_G = # TODO\n",
        "optimizer_D = # TODO Sug: usar Adam\n",
        "\n",
        "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n",
        "\n",
        "EPOCHS=25\n",
        "for epoch in range(EPOCHS):\n",
        "    for real_imgs, _ in tqdm(dataloader, desc=f\"[DCGAN] Epoch {epoch+1}/25\"):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "\n",
        "        # ======== Discriminador ========\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "\n",
        "        # ======== Generador ========\n",
        "        z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "\n",
        "\n",
        "    # Visualizar resultados\n",
        "    with torch.no_grad():\n",
        "        samples = dcgan_G(fixed_noise).detach().cpu()\n",
        "        grid = vutils.make_grid((samples + 1) / 2, nrow=8)\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"[DCGAN] Imágenes generadas - Epoch {epoch+1}\")\n",
        "        plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "2ju90HqTo7qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JN55TVVtZuS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Word Embeddings: Clasificación de Sentimientos con y sin Embeddings Preentrenados\n",
        "\n",
        "Como ya hemos visto, para entrenar modelos de clasificación de texto necesitamos representar las palabras de forma numérica para que el modelo pueda entederlas.\n",
        "Por lo visto en la clase teórica, sabemos que la mejor forma de representar las palabras es mediante **Embeddings**, es decir, vectores densos de dimensión acotada que representan las semánticas de las palabras.\n",
        "\n",
        "En la clase vimos modelos de Embeddings. Ahora queremos utilizar los embeddings pre-entrenados (que podemos obtener de Word2Vec o Glove), aprovechando que son ricos en información, para poder entrenar un modelo de clasificación.\n",
        "\n",
        "Al entrenar un modelo de clasificación de texto, tenemos básicamente tres enfoques respecto a cómo inicializar y manejar estos embeddings pre-entrenados:\n",
        "\n",
        "\n",
        "\n",
        "*   **Random**\n",
        "*   **Estático (Static)**\n",
        "* **No Estático (Non-Static)**\n",
        "\n"
      ],
      "metadata": {
        "id": "KFQbOdLlUUo5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZo_KgRfblv"
      },
      "source": [
        " **Modelo Random**\n",
        "\n",
        "En el enfoque random, no se utilizan embeddings pre-entrenados como GloVe. En cambio, se agrega una capa de embeddings al modelo que comienza con vectores inicializados aleatoriamente, normalmente usando una distribución uniforme o normal.\n",
        "\n",
        "Los embeddings se ajustan durante el entrenamiento. Es decir, los vectores van cambiando para adaptarse a la tarea específica de clasificación a medida que el modelo se entrena con los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn39207xfygD"
      },
      "source": [
        "**Modelo Estático**\n",
        "\n",
        "El enfoque estático consiste en inicializar los embeddings usando vectores pre-entrenados (en este caso, los de GloVe) y mantener estos embeddings fijos durante todo el entrenamiento del modelo.\n",
        "\n",
        "Otro enfoque sería verlo como si nuestro modelo tomara los embeddings pre-entrenados como input de su red.\n",
        "\n",
        "En la práctica, se agrega una capa de embeddings inicializada con los embeddings de GloVe, y que permanece constante (`requires_grad = False`). De esta manera los embeddings no se ven afectados durante el backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR0k3LVUgZFV"
      },
      "source": [
        "**Modelo no Estático**\n",
        "\n",
        "El enfoque no estático combina lo mejor de ambos mundos. Aquí, se inicializan los embeddings con vectores pre-entrenados (GloVe), pero se permite que se actualicen durante el entrenamiento, adaptándose así al contexto específico del dataset.\n",
        "\n",
        "Se actualizan los vectores durante el entrenamiento. Es decir, los embeddings empiezan con conocimiento previo, pero se adaptan a la tarea específica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFmlgaCg5I3"
      },
      "source": [
        "**Enfoque Generalista vs Especialista**\n",
        "\n",
        "A la hora de resolver una tarea específica, nos interesa saber el grado de adaptación que tiene un modelo para resolver dicha tarea.\n",
        "\n",
        "En general, podemos dividir a los modelos en dos clases:\n",
        "\n",
        "**Modelos Pre-entrenados (Enfoque Generalista)**\n",
        " Son modelos que han sido entrenados con corpus muy amplios, diversos, y generalmente grandes. Buscan cubrir una gran variedad de contextos y dominios, capturando relaciones semánticas generales y conocimiento lingüístico universal.\n",
        "\n",
        "**Ejemplos**: Embeddings (Word2Vec, GloVe), BERT, GPT.\n",
        "\n",
        "**Modelos Finetunneados (Enfoque Especialista)**\n",
        " Son modelos que han sido entrenados o adaptados sobre textos muy específicos de un dominio concreto. En general, se ajustan específicamente a la terminología, estilo, jerga y patrones lingüísticos de ese dominio particular.\n",
        "\n",
        "**Ejemplos**: BERT adaptado para textos clínicos, GPT especializado en resúmenes científicos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clasificación de sentimientos\n",
        "\n",
        "En el sección vamos a comparar estos paradigmas en la tarea de clasificación de sentimientos.\n",
        "\n",
        "Para ello, utilizaremos el dataset [dair-ai/emotion](https://github.com/dair-ai/emotion_dataset?tab=readme-ov-file).\n",
        "\n",
        "el objetivo de esta parte es entrenar tres modelos:\n",
        "\n",
        "*   Un modelo inicializado con embeddings random\n",
        "*   Un modelo que utilice los embeddings de GloVe como embeddings estáticos.\n",
        "* Un modelo que utilice los embeddings de GloVe como embeddings no estáticos.\n",
        "\n",
        "Para ello utilizaremos una [CNN Text Classification Network](https://aclanthology.org/D14-1181.pdf), que usa filtros especiales (llamados convolucionales) para analizar el texto, identificar patrones importantes y realizar la clasificación de sentimientos con precisión."
      ],
      "metadata": {
        "id": "GPvHyH3DYx62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3VAGVVm16kS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2d655b47-7a30-4cfc-8b01-06bc73d0a944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets flair gensim -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUq4-00_FsPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaab083-594b-455e-a1b7-57ce5e81d4d5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m187.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m197.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy==1.26.4 pandas==2.2.1 --force-reinstall --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpfMKr3JfbpQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.data import Sentence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgF_NkEqmRvC"
      },
      "source": [
        "### 3.1. Cargando los embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSyW3FSl011x"
      },
      "outputs": [],
      "source": [
        "def custom_embed_sentence(sentence_text, glove_embedding):\n",
        "    \"\"\"\n",
        "    Convierte una oración en embeddings usando un modelo GloVe preentrenado.\n",
        "\n",
        "    Args:\n",
        "        sentence_text (str): Texto de la oración a procesar\n",
        "        glove_embedding: Modelo de embeddings GloVe de Flair\n",
        "\n",
        "    Returns:\n",
        "        embeddings : Tensor 2D con shape (num_tokens, embedding_dim)\n",
        "                     donde cada fila es el embedding de un token.\n",
        "                     Retorna tensor vacío si no hay tokens.\n",
        "    \"\"\"\n",
        "    sentence = Sentence(sentence_text)\n",
        "    glove_embedding.embed(sentence)\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk1ryoVJ3DmX"
      },
      "outputs": [],
      "source": [
        "glove_embeddings = WordEmbeddings('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tag1lLUeFWJL"
      },
      "outputs": [],
      "source": [
        "custom_embed_sentence('hi how are you', glove_embeddings).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1i41Bp_mPiE"
      },
      "source": [
        "### 3.2 Cargando el Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV7rVWMj2-XO"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "train_texts[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4p3pCbr35MN"
      },
      "outputs": [],
      "source": [
        "sentence_text = \"i am feeling grouchy\"\n",
        "sentence = Sentence(sentence_text)\n",
        "glove_embeddings.embed(sentence)\n",
        "\n",
        "print(\"Sentece:\", sentence_text)\n",
        "print(\"Word Embeddings:\")\n",
        "for token in sentence:\n",
        "  print(\"Word: \", token.text)\n",
        "  print(\"Embedding shape:\", token.embedding.shape)\n",
        "  print(\"Embedding vector: \", token.embedding[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K-vS6xZl_zZ"
      },
      "source": [
        "**Preparando la data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ2-n0Sz4lOW"
      },
      "outputs": [],
      "source": [
        "emotion_to_label = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
        "label_to_emotion = {v: k for k, v in emotion_to_label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vMeBYlkPYWt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Armando el vocabulario\n",
        "vocab = set()\n",
        "for text_sentence in train_texts:\n",
        "  sentence = Sentence(text_sentence)\n",
        "  for token in sentence:\n",
        "    vocab.add(token.text)\n",
        "\n",
        "vocab.add('<UNK>')\n",
        "vocab.add('<PAD>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOlr1w_QrXjA"
      },
      "outputs": [],
      "source": [
        "word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM_vh4Q2Zh2T"
      },
      "outputs": [],
      "source": [
        "text = 'i am very happy'\n",
        "sentence = Sentence(text)\n",
        "tokens = [token for token in sentence]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaUBoNyAlzOn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, word2idx, max_length=30):\n",
        "      \"\"\"\n",
        "      Inicializa el dataset de análisis de sentimientos.\n",
        "\n",
        "      Args:\n",
        "          texts (list): Lista de textos/oraciones para clasificar\n",
        "          labels (list): Lista de etiquetas correspondientes a cada texto\n",
        "          vocab (dict): Vocabulario completo (puede no usarse directamente)\n",
        "          word2idx (dict): Diccionario que mapea palabras a índices numéricos\n",
        "          max_length (int, optional): Longitud máxima de secuencia. Por defecto 30.\n",
        "      \"\"\"\n",
        "      #TODO\n",
        "    def __len__(self):\n",
        "      \"\"\"\n",
        "      Retorna el número total de elementos en el dataset.\n",
        "\n",
        "      Returns:\n",
        "          int: Cantidad de textos/muestras en el dataset\n",
        "      \"\"\"\n",
        "      # TODO\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      \"\"\"\n",
        "      Obtiene un elemento del dataset por su índice.\n",
        "\n",
        "      Procesa el texto tokenizándolo, convirtiéndolo a índices numéricos,\n",
        "      y aplicando padding/truncation para mantener longitud fija.\n",
        "\n",
        "      Args:\n",
        "          idx (int): Índice del elemento a obtener\n",
        "\n",
        "      Returns:\n",
        "          tuple: (tensor_indices, tensor_label) dtype=torch.long\n",
        "              - tensor_indices: tensor con índices de palabras (longitud max_length)\n",
        "              - tensor_label: tensor con la etiqueta correspondiente\n",
        "      \"\"\"\n",
        "      # TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfoQOq4V1dQA"
      },
      "outputs": [],
      "source": [
        "sentiment_dataset = SentimentDataset(train_texts, train_labels, vocab, word2idx)\n",
        "train_dataset, val_dataset = train_test_split(sentiment_dataset, test_size=0.4, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il1mIo8zN-XD"
      },
      "source": [
        "### 3.3. Construyendo la matriz de embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaptIFJODtT"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_dim = glove_embeddings.embedding_length\n",
        "\n",
        "embedding_matrix = torch.zeros((vocab_size, embed_dim), dtype=np.float32)\n",
        "for word in vocab:\n",
        "    sentence = Sentence(word)\n",
        "    glove_embeddings.embed(sentence)\n",
        "    embedding_vector = sentence[0].embedding.cpu().numpy()\n",
        "    i=word2idx[word]\n",
        "    embedding_matrix[i,:] = embedding_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBHcg3jaTbID"
      },
      "outputs": [],
      "source": [
        "def look_up_table(word_idx):\n",
        "    x = torch.zeros(vocab_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9qvwbrltur"
      },
      "source": [
        "### 3.4. Creando los modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QHjhq3nOdSp"
      },
      "source": [
        "Completar el código para modificar el comportamiento de la red en base al `model_type` que puede ser `rand`, `static` o `non-static`.\n",
        "\n",
        "Explorar la clase `nn.Embedding`. [Link](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) a la documentación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhxbz-7sXn2B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.data import Sentence\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HighLevelTextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, class_num, filter_sizes, num_filters,\n",
        "                 dropout, model_type='non-static', pretrained_embeddings=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: Size of the vocabulary.\n",
        "            embed_dim: Dimensionality of word embeddings.\n",
        "            class_num: Number of output classes.\n",
        "            filter_sizes: List of filter (kernel) sizes (e.g., [3,4,5]).\n",
        "            num_filters: Number of filters per filter size.\n",
        "            dropout: Dropout rate.\n",
        "            model_type: 'rand', 'static', or 'non-static'\n",
        "            pretrained_embeddings: numpy array of shape (vocab_size, embed_dim) or None.\n",
        "        \"\"\"\n",
        "        super(HighLevelTextCNN, self).__init__()\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "          if model_type # TODO\n",
        "          # TODO\n",
        "\n",
        "\n",
        "        # Build convolutional blocks for each filter size\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(1, num_filters, kernel_size=(fs, embed_dim)),\n",
        "                nn.ReLU()\n",
        "            ) for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (batch_size, sentence_length)\n",
        "        \"\"\"\n",
        "        # 1. Embedding: (batch_size, sentence_length, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "        # 2. Add channel dimension: (batch_size, 1, sentence_length, embed_dim)\n",
        "        x = x.unsqueeze(1)\n",
        "        conv_results = []\n",
        "        for conv in self.conv_blocks:\n",
        "            conv_out = conv(x)          # (batch_size, num_filters, L, 1)\n",
        "            conv_out = conv_out.squeeze(3)  # (batch_size, num_filters, L)\n",
        "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2)).squeeze(2)  # (batch_size, num_filters)\n",
        "            conv_results.append(pooled)\n",
        "        cat = torch.cat(conv_results, 1)  # (batch_size, num_filters * len(filter_sizes))\n",
        "        drop = self.dropout(cat)\n",
        "        logits = self.fc(drop)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzBgE1kbQmG9"
      },
      "outputs": [],
      "source": [
        "# @title Hiperparámetros\n",
        "class_num = len(emotion_to_label)\n",
        "filter_sizes = [3, 4]\n",
        "num_filters = 50\n",
        "dropout = 0.2\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQGfmYKERPMM"
      },
      "outputs": [],
      "source": [
        "# @title Instanciando los modelos\n",
        "\n",
        "model_rand = HighLevelTextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    class_num=class_num,\n",
        "    filter_sizes=filter_sizes,\n",
        "    num_filters=num_filters,\n",
        "    dropout=dropout,\n",
        "    model_type='rand',\n",
        "    pretrained_embeddings=None\n",
        ")\n",
        "\n",
        "\n",
        "model_static = HighLevelTextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    class_num=class_num,\n",
        "    filter_sizes=filter_sizes,\n",
        "    num_filters=num_filters,\n",
        "    dropout=dropout,\n",
        "    model_type='static',\n",
        "    pretrained_embeddings=embedding_matrix\n",
        ")\n",
        "\n",
        "model_non_static = HighLevelTextCNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    class_num=class_num,\n",
        "    filter_sizes=filter_sizes,\n",
        "    num_filters=num_filters,\n",
        "    dropout=dropout,\n",
        "    model_type='non-static',\n",
        "    pretrained_embeddings=embedding_matrix\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezNiS6n-lKVI"
      },
      "source": [
        "### 3.5 Training Loop\n",
        "\n",
        "Explicar qué hace este loop de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQwzonbXtAeh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, model_type):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss() #Ya incluye la softmax adentro\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses, val_f1 = [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "\n",
        "        tqdm_train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [TRAIN]\", leave=False)\n",
        "\n",
        "        for batch_inputs, batch_labels in tqdm_train_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        tqdm_val_loader = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [VAL]\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in tqdm_val_loader:\n",
        "                batch_inputs = batch_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                outputs = model(batch_inputs)\n",
        "                loss = criterion(outputs, batch_labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += batch_labels.size(0)\n",
        "                correct += predicted.eq(batch_labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"model_{model_type} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SydLPDYOtXxK"
      },
      "outputs": [],
      "source": [
        "train_loss_rand, val_loss_rand = train_model(model_rand, train_loader, val_loader, num_epochs, 'rand')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPBl4hAcaama"
      },
      "outputs": [],
      "source": [
        "Lotrain_loss_static, val_loss_static = train_model(model_static, train_loader, val_loader, num_epochs, 'static')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNKaZCTTu30z"
      },
      "outputs": [],
      "source": [
        "train_loss_non_static, val_loss_non_static = train_model(model_non_static, train_loader, val_loader, num_epochs, 'non-static')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYBJ_cTXp2Ez"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses, model_type):\n",
        "  plt.plot(train_losses, label=f'Train Loss')\n",
        "  plt.plot(val_losses, label=f'Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(f'{model_type} Model Training and Validation Losses')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPkFHDuHml_M"
      },
      "source": [
        "##### Aca idealmente tenemos las curvas de entrenamiento para los tres modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7WALl86uPEU"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_loss_rand, val_loss_rand, 'Random Non Static')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQI1G8QidIE-"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_loss_static, val_loss_static, 'Static')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm0zKEqOyX6J"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_loss_non_static, val_loss_non_static, 'Non Static')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlsPWwRTdO9j"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(train_loss_rand) + 1)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, train_loss_rand, label='Random Non-Static Model')\n",
        "plt.plot(epochs, train_loss_static, label='Static Model')\n",
        "plt.plot(epochs, train_loss_non_static, label='Non-Static Model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Curves for 3 Models')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HStSICxOdV06"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, val_loss_rand, label='Random Non-Static Model')\n",
        "plt.plot(epochs, val_loss_static, label='Static Model')\n",
        "plt.plot(epochs, val_loss_non_static, label='Non-Static Model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss Curves for 3 Models')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqXMMKHGXFNk"
      },
      "outputs": [],
      "source": [
        "def generate_classification_report_table(model, dataset, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in tqdm(dataset, desc=\"Generating predictions\"):\n",
        "            embeddings = embeddings.to(device)\n",
        "            outputs = model(embeddings)\n",
        "            _, predicted = outputs.max(1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(all_labels, all_predictions, target_names=emotion_to_label.values(), output_dict=True)\n",
        "\n",
        "    # Convert the report to a DataFrame\n",
        "    df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Reorder the columns\n",
        "    df = df[['precision', 'recall', 'f1-score', 'support']]\n",
        "\n",
        "    # Format the values\n",
        "    df['precision'] = df['precision'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['recall'] = df['recall'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['f1-score'] = df['f1-score'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['support'] = df['support'].astype(int)\n",
        "\n",
        "    # Print the table\n",
        "    print(df.to_string())\n",
        "\n",
        "# Add this to your main function or after training\n",
        "def evaluate_model(model, val_dataset):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "    generate_classification_report_table(model, val_loader, device)\n",
        "\n",
        "# Call this function after training your model\n",
        "evaluate_model(model_non_static, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYTB2IH3msNi"
      },
      "source": [
        "### 3.6 Comparando las performances\n",
        "\n",
        "¿Qué observa?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA6Iy1wDeVMs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "test_texts = dataset['test']['text']\n",
        "test_labels = dataset['test']['label']\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, vocab, word2idx)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "models = [model_rand, model_static, model_non_static]\n",
        "model_names = ['Random Model', 'Static Model', 'Non-Static Model']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "reports = {}\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_predictions, target_names=list(emotion_to_label.values()), output_dict=True)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "    reports[name] = {\"classification_report\": report, \"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "for name, metrics in reports.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {metrics['f1']:.4f}\")\n",
        "    df = pd.DataFrame(metrics[\"classification_report\"]).transpose()\n",
        "\n",
        "    df = df[['precision', 'recall', 'f1-score', 'support']]\n",
        "    df['precision'] = df['precision'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['recall'] = df['recall'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['f1-score'] = df['f1-score'].apply(lambda x: f\"{x:.6f}\")\n",
        "    df['support'] = df['support'].astype(int)\n",
        "    print(df.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "ZnTan46gZqwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Apéndice\n",
        "\n",
        "#### Referencias bibliográficas\n",
        "- https://lilianweng.github.io/posts/2018-08-12-vae/\n",
        "- Hand-On Machine Learning, Geron A.\n",
        "- Probabilistic Machine Learning: An Introduction, Murphy K.\n",
        "- Deep Learning:  Foundations and Concepts, Bishop H. & Bishop C.\n",
        "\n",
        "#### Fuentes\n",
        "- https://colab.research.google.com/github/ageron/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb#scrollTo=R1WIZRNByLhL\n",
        "- https://colab.research.google.com/drive/1r3InSYsSN6BgZdnyCu3vCnpZ1cniKRTJ?usp=sharing\n",
        "\n"
      ],
      "metadata": {
        "id": "bFG_NcTYenAh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DFyS5obeuYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}